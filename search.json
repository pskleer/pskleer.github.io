[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Aspects in Econometrics - Python II module",
    "section": "",
    "text": "1 About",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Computational Aspects in Econometrics - Python II module",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\nWelcome to the online “book” for the Python II module of Computational Aspects in Econometrics. We will follow the content in this book during the lectures and it is the basis of the material that will appear on the exam, so you should read through this book carefully. Because this book is new, it is likely that we will make some edits throughout the course.\nPlease not that this is an advanced Python module. We assume familiarity with the basics of programming in Python. For students enrolled in the bachelor Econometrics and Operations Research, the topics of the course Programming for EOR is a good example of what I expect you to be familiar with. An online book covering most of these topis can be found here. This book also contains some topics not covered in Programming for EOR.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Computational Aspects in Econometrics - Python II module",
    "section": "1.2 Goal",
    "text": "1.2 Goal\nThe goal of this module is to teach you the basics of scientific computing with Python. Here you should think mostly of implementing algorithmic tasks that you encounter during your Econometrics and Operations Research courses, such as, linear algebra, optimization, statistics and machine learning. We hope that the skills you are taught here can be useful for, e.g., numerical work in your bachelor or (perhaps later) master thesis. Furthermore, many companies nowadays program in Python, so the topics of this module can also be useful in your professional career later in life.\nNext to teaching you how to implement certain mathematical task in a correct manner in Python, we also put emphasis on good coding practices. Especially if you start to write scripts with hundres of lines of code, it is important that you learn how to do this in a structured fasion using efficient Python functionality. Good coding practices are the topic of the next chapter. The idea is that you use these practices when doing the exercises corresponding to every lecture, as well as the group assignment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "index.html#use-of-spyder",
    "href": "index.html#use-of-spyder",
    "title": "Computational Aspects in Econometrics - Python II module",
    "section": "1.3 Use of Spyder",
    "text": "1.3 Use of Spyder\nThis course document is based on the use of Spyder as integrated development environment (IDE) for creating Python code, i.e., the program that the code is written in. You can also use VS code or any other IDE to do the exercises and/or assignment in. Whenever this book contains screenshots to illustrate something, they will have been taken in Spyder.\nTo install the Anaconda distribution containing Spyder, follow the steps here. This is the installation that was alos recommended in the Python I module.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html",
    "href": "02-good-coding.html",
    "title": "2  Good coding practices",
    "section": "",
    "text": "2.1 Efficient computations\nPerhaps the most important topic of this course is that of efficient computation, i.e., to make efficient use of the mathematical functionality that Python has to offer, in particular the functionality of the NumPy package.\nThe way to think of this is as follows: Many of the mathematical tasks and exercises that we will see could, in theory, be solved using for- and/or while-loops, as well as if/else-statements. However, there are often more efficient functions programmed in the NumPy package in Python that can carry out these task in a quicker way using less code!\nLet us look at an example. Suppose I am given a vector x = [x_1,\\dots,x_n] \\in \\mathbb{R}^n, and I want to compute the L^2-norm \n||x||_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n of this vector. One way to solve this directly would be by using a for-loop to compute the sum inside the square root, and then take the square root of this number. We will illustrate this next for the vector x = [1,2,3,\\dots,300.000].\n# Length of vector\nn = 300000\n\n# Compute inner summation of ||x||_2\ninner_sum = 0\nfor i in range(1,n+1):\n    inner_sum = inner_sum + i**2\n\n# Take square root\ntwo_norm = (inner_sum)**(0.5)\n\n# Print value of two-norm\nprint(two_norm)\n\n94868566.97584295\nAnother way to do this is to define the vector x efficiently making use of the arange() function in Numpy, followed by the linalg.norm() function that can compute the L^2-norm for us directly. We will see these functions in more detail in a subsequent chapter.\nIt is standard convention to import the Numpy package under the alias np.\n#Import Numpy package under the alias np\nimport numpy as np\n\n# Length of vector\nn = 300000\n\n# Define vector x as array using arange() function\nx = np.arange(1,n+1)\n\n# Compute two-norm with built-in function\ntwo_norm = np.linalg.norm(x)\n\n# Print value of two-norm\nprint(two_norm)\n\n94868566.97584295\nThis is a much cleaner, and in fact faster, way to compute the L^2-norm of the vector x. Especially for large values of n, the second piece of code that exploits the Numpy functionality can be much faster! The Python code below (which we will not discuss) illustrates this comparison by timing how long both approaches require to compute the norm for n = 30.000.000 (i.e., thirty million). If you run the code below on your own device, you might get different outputs, but, in general, there should be a significant (multiplicative) difference in execution time.\nCode that times execution of the two approaches above\nimport numpy as np\nimport time\n\n## Computing norm with for-loop\n\n# We determine the time it takes to input the code between start and end\nstart = time.time()\n\n# Length of vector\nn = 30000000\n\n# Compute inner summation of ||x||_2\ninner_sum = 0\nfor i in range(1,n+1):\n    inner_sum = inner_sum + i**2\n\n# Take square root\ntwo_norm = (inner_sum)**(0.5)\n\nend = time.time()\n\n# Time difference\nloop_time = end - start\nprint('%.10f seconds needed for computing norm' % (loop_time), \n      'with for-loop')\n\n## Computing norm with Numpy functions\n\n#We determine the time it takes to input the code between start and end\nstart = time.time()\n\n# Length of vector\nn = 30000000\n\n# Define vector x as array using arange() function\nx = np.arange(1,n+1)\n\n# Compute two-norm with built-in function\ntwo_norm = np.linalg.norm(x)\n\nend = time.time()\n\n# Time difference\nnumpy_time = end - start\nprint('%.10f seconds needed for computing norm' % (numpy_time), \n      'with Numpy functions')\n\n\n#This shows how many times faster Numpy approach is faster than for-loop solution\nif numpy_time != 0:\n    (print('NumPy\\'s approach is %i times more efficient than for-loop approach.'\n            % ((loop_time)/(numpy_time))))\n\n\n21.3754873276 seconds needed for computing norm with for-loop\n0.2553257942 seconds needed for computing norm with Numpy functions\nNumPy's approach is 83 times more efficient than for-loop approach.\nOne important take-away of the above comparison is the following.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html#efficient-computations",
    "href": "02-good-coding.html#efficient-computations",
    "title": "2  Good coding practices",
    "section": "",
    "text": "When performing mathematical tasks, avoid the use of for- and while-loops, as well as if/else-statements, by efficient use of Python functionality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html#no-hard-coding",
    "href": "02-good-coding.html#no-hard-coding",
    "title": "2  Good coding practices",
    "section": "2.2 No hard coding",
    "text": "2.2 No hard coding\nSuppose we are given the function f(x) = a \\cdot x^2+ a\\cdot b \\cdot x - a \\cdot b \\cdot c and the goal is to compute f(10) for a = 3, b = 4 and c = -10. One way of doing this would be to plug in all the variables and return the resulting function value.\n\n# Hard coding\nprint(3*(10**2) + 3*4*10 + 4*-2*3)\n\n396\n\n\nHowever, this is inefficient for the following reason: If we would want to change the number x = 10 to, e.g., x = 20, we would have to twice replace 10 by 20. The same is true for the variable a, which appears in even three places. In general, in large scripts, variables can appear in more than a hundred places. To overcome this inefficiency issue, it is always better to separate the input data (the x,a,b and c in this case) from the function execution\n\n# No hard coding\ndef f(x, a, b, c):\n    return a*x**2 + a*b*x + b*c*a\n\nx = 10\na, b, c = 3, 4, -2\nprint(f(x, a, b, c))\n\n396\n\n\nThe take-away of the above comparison is the following.\n\nSeparate input data from the execution of functions and commands.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html#dont-repeat-yourself-dry",
    "href": "02-good-coding.html#dont-repeat-yourself-dry",
    "title": "2  Good coding practices",
    "section": "2.3 Don’t repeat yourself (DRY)",
    "text": "2.3 Don’t repeat yourself (DRY)\nIf you have to carry out a piece of code for multiple sets of input data, always avoid copy-pasting the code. As a first step, try to to the repeated execution using a for-loop. For example, suppose we want to print the function values f(x,3,4,-2) for x = 1,2,3,4, with f as in the previous section. We can do this as follows.\n\ndef f(x, a, b, c):\n    return a*x**2 + a*b*x + b*c*a\n\na, b, c = 3, 4, -2\n\n# With repetition\nx = 1\nprint('%.2f' % f(x,a,b,c))\nx = 2\nprint('%.2f' % f(x,a,b,c))\nx = 3\nprint('%.2f' % f(x,a,b,c))\nx = 4\nprint('%.2f' % f(x,a,b,c))\n\n-9.00\n12.00\n39.00\n72.00\n\n\nA more efficient way of typing this, is by using a for-loop that repeatedly executes the print statement.\n\nimport numpy as np\n\na, b, c = 3, 4, -2\nx = [1,2,3,4]\n# Determine all function values with list comprehension\ny = [f(i,a,b,c) for i in x]\n\n# Print values in y (could also print the whole vector y right away)\nfor j in y:\n    print(j)\n\n-9\n12\n39\n72\n\n\nThe take-away of the above comparison is the following.\n\nDon’t carry out the same task on different inputs twice by copy-pasting code, but use, e.g., a for-loop in which you iterate over the inputs.\n\nIn fact, later on in the course we will explain the concept of vectorizing a function, to make sure it can handle multiple input simultaneously. This is another way to avoid unnecessary repetition and in fact also the use of loops.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html#sec-newton-method",
    "href": "02-good-coding.html#sec-newton-method",
    "title": "2  Good coding practices",
    "section": "2.4 Single responsibility",
    "text": "2.4 Single responsibility\nWhen you write larger pieces of codes, it is often useful to split it up in smaller parts that all serve their own purpose. Suppose we want to implement Newton’s method for finding a root x of a function f : \\mathbb{R} \\rightarrow \\mathbb{R}, i.e., a point x that satisfies f(x) = 0.\nNewton’s methods starts with a (suitably chosen) initial guess x_0 \\in \\mathbb{R} and repeatedly computes better approximations using the recursive formula \nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}\n\nThe goal is to implement this formula for the function f(x) = (x-1)^2 - 1, whose derivative is f'(x) = 2(x-1). The roots of this function are x = 0 and x = 2.\n\n# We implement the function y = x - f(x)/f'(x)\ndef y(x):\n    return x - ((x-1)**2 - 1)/(2*(x-1))\n\nx0 = 3\nx1 = y(x0) \n\nprint(x1)\n\n2.25\n\n\nA clearer way to implement the formula x_{i+1} = x_i - f(x_i)/f'(x_i) is to define separate functions for the evaluation of the functions f and f', and then combine these to create the recursive formula. In this way, we get three functions that are each responsible for one aspect of Newton’s formula, namely implementing the function f, implementing the function f' and computing the recursive formula. This also makes it easier for a user of the code to understand what is happening.\n\n# Single responsibility version\n\n# Define f\ndef f(x):\n    return (x-1)**2 - 1\n\n# Define f'\ndef fprime(x):\n    return 2*(x-1)\n\n# Implement function y = x - f(x)/f'(x)\ndef y(x):\n    return x - f(x)/fprime(x)\n\nx0 = 3\nx1 = y(x0) \n\nprint(x1)\n\n2.25\n\n\nThe take-away of the above comparison is the following.\n\nIf you are implementing a complex mathematical task involving multiple aspects, try to separate these aspects in different functions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "02-good-coding.html#documentation",
    "href": "02-good-coding.html#documentation",
    "title": "2  Good coding practices",
    "section": "2.5 Documentation",
    "text": "2.5 Documentation\nThe final good coding practice that we discuss is documentation. Ideally, you should always explain inside a function what the inputs and outputs are, and for larger scripts it is good to indicate what every part of the script does. For smaller functions and scripts this is not always necessary. We will next give an example for Newton’s method as in the previous section\n\n## Function that implements Newton's method\ndef newton(f,fprime,x0,iters):\n    \"\"\"\n    This function implements Newton's iterative method \n    x_{i+1} = x_i - f(x_i)/f'(x_i) for finding root of f.\n    \n    Input parameters\n    ----------\n    f : Function f\n    fprime : Derivative of the function f\n    x0 : Initial estimate for root x of f.\n    iters : Number of iterations that we run Newton's method.\n    \n    Returns\n    -------\n    Approximation for x satisfying f(x) = 0.\n    \"\"\"\n    \n    # Initial guess\n    x = x0 \n    \n    # Repeatedly compute the recursive formula \n    # by overwriting x for 'iters' iterations\n    for i in range(iters):\n        x_new = x - f(x)/fprime(x) \n        x = x_new\n    return x\n    \n## An example of Newton's method as implemented above\n\n# Define f\ndef f(x):\n    return (x-1)**2 - 1\n\n# Define f'\ndef fprime(x):\n    return 2*(x-1)\n    \n# Define intial guess and number of iterations\nx0 = 10\niters = 6   \n\n# Run Newton's method\nroot = newton(f,fprime,x0,iters)\n\n# Print output and explain what has been computed\nprint('Running Newton\\'s method for %.i iterations with initial' % iters,\n       'estimate %.2f' % x0,'\\n','gives (estimated) root x = %.7f' % root, \n       'with f(x) = %.7f' % f(root))\n\nRunning Newton's method for 6 iterations with initial estimate 10.00 \n gives (estimated) root x = 2.0000013 with f(x) = 0.0000025\n\n\nThe following is a set of guidelines regarding how to add documentation to a function.\n\nTry to adhere to the following documention rules when writing complex functions:  1. Function documentation between triple double-quote characters. 2. Clearly describe what a function does and what its input and output arguments are. 3. Choose descriptive variable names, lines not longer than 80 characters. 4. Don’t add comments for every line. Add comments for main ideas and complex parts.  5. A comment should not repeat the code as text (e.g. “time = time + 1 # increase time by one).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good coding practices</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html",
    "href": "03-numpy-arrays.html",
    "title": "3  NumPy arrays",
    "section": "",
    "text": "3.1 Introduction\nThe NumPy package (module) is used in almost all numerical computations using Python. It is a package that provides high-performance vector, matrix and higher-dimensional data structures for Python. High-performance here refers to the fact that Python can perform computations on such data structures very quickly if appropriate functions are used for this.\nTo use NumPy you need to import the numpy module. This is typically done under the alias np so that you don’t have to type numpy all the time when using a function from the module.\nimport numpy as np\nWe emphasize at this point that there is often not a unique way or command to achieve a certain outcome. When doing the exercises corresponding to the theory given in this chapter, it is, however, recommended to find a solution using the presented functionality.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#creating-arrays",
    "href": "03-numpy-arrays.html#creating-arrays",
    "title": "3  NumPy arrays",
    "section": "3.2 Creating arrays",
    "text": "3.2 Creating arrays\nIn the NumPy package the data type used for vectors, matrices and higher-dimensional data sets is an array. There are a number of ways to initialize new arrays, for example from\n\na Python list or tuples;\nusing functions that are dedicated to generating numpy arrays, such as arange() and linspace() (we will see those later);\nreading data from files.\n\n\n3.2.1 Lists\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array() function. Since we imported NumPy under the alias np, we use np.array() for this.\nTo create a vector, the argument to the array function is a Python list\n\nv = np.array([1,2,3,4]) #Array creation from list [1,2,3,4]\nprint(v)\n\n[1 2 3 4]\n\n\nTo create a matrix, the argument to the array function is a nested Python list. Every element of the outer list is a list corresponding to a row of the matrix. For example, the matrix \nM = \\left[ \\begin{matrix}1 & 2 & 7\\\\ 3 & -4 & 4 \\end{matrix} \\right]\n is created as follows.\n\nM = np.array([[1, 2, 7], [3, -4, 4]])\nprint(M)\n\n[[ 1  2  7]\n [ 3 -4  4]]\n\n\nYou can access the shape (number of rows and columns) , size (number of elements) and number of dimensions (number of axes in matrix) of the array with the shape, size and ndim attributes, respectively. Note that the size is simply the product of the numbers in the shape tuple, and the number of dimensions is the size of the shape tuple.\n\n# Shape of matrix M\nshape_M = M.shape  #np.shape(M) also works\nprint(shape_M) \n\n(2, 3)\n\n\n\n# Size of matrix M\nsize_M = M.size  #np.size(M) also works\nprint(size_M) \n\n6\n\n\n\n# Number of diemenions\nndim_M = M.ndim  #np.ndim(M) also works\nprint(ndim_M) \n\n2\n\n\nNumPy arrays are of the type ndarray (short for n-dimensional array). You can access this type through the type() function.\n\n# Type of matrix M\ntype_M = type(M)\nprint(type_M) \n\n&lt;class 'numpy.ndarray'&gt;\n\n\nSo far a NumPy array looks awfully much like a Python list (or nested list). Why not simply use Python lists for computations instead of creating a new array type?\nThere are several reasons:\n\nPython lists are very general. They can contain any kind of object. They are dynamically typed. They do not support mathematical functions such as matrix and dot multiplications.\nNumpy arrays are statically typed and homogeneous. The type of the elements is determined when the array is created.\nNumpy arrays are memory efficient.\nBecause of the static typing, fast implementation of mathematical functions such as multiplication and addition of numpy arrays can be implemented in a compiled language (C and Fortran are used).\n\nUsing the dtype (data type) attribute of an array, we can see what type the data inside an array has.\n\n# Data type of elements in array\ndtype_M = M.dtype\nprint(dtype_M)\n\nint32\n\n\nIf we want, we can explicitly define the type of the array data when we create it, using the dtype keyword argument:\n\n# Define data as integers\nM = np.array([[1, 2], [3, 4]], dtype=int)\nprint('M = \\n', M)\n\nM = \n [[1 2]\n [3 4]]\n\n\n\n# Define data as floats\nN = np.array([[1, 2], [3, 4]], dtype=float)\nprint('N = \\n', N)\n\nN = \n [[1. 2.]\n [3. 4.]]\n\n\n\n# Define data as complex floats\nO = np.array([[1, 2], [3, 4]], dtype=complex)\nprint('O = \\n', O)\n\nO = \n [[1.+0.j 2.+0.j]\n [3.+0.j 4.+0.j]]\n\n\nCommon data types that can be used with dtype are: int, float, complex, bool, object, etc.\nWe can also explicitly define the bit size of the data types, such as: int64, int16, float128, complex128. For example, int64 allows us to define an integer variable in the range [−264,…,264].\nYou can also change the data type of the elements using the astype() method.\n\nM = np.array([[1,2], [3,4]])\nprint(M.dtype)\n\nint32\n\n\n\n# Define M_float as matrix whose elements are those of \n# the matrix M, but then as floats.\nM_float = M.astype(float)\nprint(M_float)\n\n[[1. 2.]\n [3. 4.]]\n\n\n\nprint(M_float.dtype)\n\nfloat64\n\n\n\n\n3.2.2 Arrays from functions\nThere are various useful arrays that can be automatically created using functions from the NumPy package. These arrays are typically hard to implement directly as a list.\narange(n): This function creates the array [0,1,2,\\dots,n-1] whose elements range from 0 to n-1.\n\nn = 10\nx = np.arange(n) \n\nprint(x)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\nIf you want to explicitly define the data type of the elements, you can add the dtype keyword argument (the same applies for all functions that are given below).\n\nn = 10\nx = np.arange(n, dtype=float) \n\nprint(x)\n\n[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n\n\narange(a,b): This function creates the array [a,a+1,a+2,\\dots,b-2,b-1].\n\na, b = 5,11\nx = np.arange(a,b)\n\nprint(x)\n\n[ 5  6  7  8  9 10]\n\n\narange(a,b,step): This function creates the array [a,a+step,a+2\\cdot step,\\dots,b-2\\cdot step,b-step]. That is, the array ranges from a to b (but not including b itself), in steps of size step.\n\na, b, step = 5, 11, 0.3\nx = np.arange(a,b,step)\n\nprint(x)\n\n[ 5.   5.3  5.6  5.9  6.2  6.5  6.8  7.1  7.4  7.7  8.   8.3  8.6  8.9\n  9.2  9.5  9.8 10.1 10.4 10.7]\n\n\nlinspace(a,b,k): Create a discretization of the interval [a,b] containing k evenly spaced points, including a and b as the first and last element of the array.\n\na,b,k = 5,10,20\nx = np.linspace(a,b,k)\n\nprint(x)\n\n[ 5.          5.26315789  5.52631579  5.78947368  6.05263158  6.31578947\n  6.57894737  6.84210526  7.10526316  7.36842105  7.63157895  7.89473684\n  8.15789474  8.42105263  8.68421053  8.94736842  9.21052632  9.47368421\n  9.73684211 10.        ]\n\n\ndiag(x): This function creates a matrix whose diagonal contains the list/vector/array x.\n\nx = np.array([1,2,3])\nD = np.diag(x)\n\nprint(D)\n\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n\n\nnp.zeros(n): This function create a vector of length n with zeros.\n\nn = 5\nx = np.zeros(n)\n\nprint(x)\n\n[0. 0. 0. 0. 0.]\n\n\nnp.zeros((m,n)): This function create a matrix of size m \\times n with zeros. Note that we have to input the size of the matrix as a tuple (m,n); using np.zero(m,n)\n\nm, n = 2, 5\nM = np.zeros((m,n))\n\nprint(M)\n\n[[0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]\n\n\nnp.ones(n) and np.ones((m,n)): These functions create a vector of length n with ones, and a matrix of size m \\times n with ones, respectively.\n\nm, n = 2, 5\nx = np.ones(n)\n\nprint(x)\n\n[1. 1. 1. 1. 1.]\n\n\n\nM = np.ones((m,n))\n\nprint(M)\n\n[[1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]]\n\n\n\n\n3.2.3 Reading data from files\nThe third option, which you might use most often in a professional context, is to read in data from a file directly into a NumPy array. You can do this using the loadtxt() function.\nIf you want to try this yourself, download the file numerical_data.dat here and store it in the same folder as where you are storing the Python script in which you execute the code snippet below.\n\n# Load data into NumPy array\ndata_dat = np.loadtxt('numerical_data.dat')\n\n# Print the data\nprint(data_dat)\n\n[[ 1.   5.   4.  -9.   1. ]\n [ 3.   5.   6.   7.   7. ]\n [ 4.   3.   2.   1.   0.5]]\n\n\nPython puts every row in the data (DAT) file into a separate row of the NumPy array; note that the numbers in the data file are separated by a whitespace character.\nWe can also save data from a Numpy array into a DAT-file using the savetxt() function. The first argument of this function is the name of the file in which you want to store the array, and the second argument is the array to be stored.\n\n# Matrix M\nM = np.array([[1,2,3],[5,6,7],[10,11,12],[14,15,16]])\n\n# Save matrix to DAT file\nnp.savetxt('matrix.dat', M)\n\nThis should have created the file matrix.dat in the same folder as where you stored the Python script that ran the code above. You might notice that the numbers are stored using the scientific notation. For example, the number 1 appears as 1.000000000000000000e+00 in the CSV-file.\nYou can suprress this behaviour by explicitly specifying the data type in which you want the numbers in the matrix to be stored using the fmt keyword argument. For example, fmt = '%i' stores the numbers as integers.\n\n# Matrix M\nM = np.array([[1,2,3],[5,6,7],[10,11,12],[14,15,16]])\n\n# Save matrix to DAT file\nnp.savetxt('matrix_int.dat', M, fmt='%i')\n\nThis should have created the file matrix_int.dat in the same folder as where you stored the Python script that ran the code above.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#accessing",
    "href": "03-numpy-arrays.html#accessing",
    "title": "3  NumPy arrays",
    "section": "3.3 Accessing",
    "text": "3.3 Accessing\nIn this section we will describe how you can access, or index, the data in a NumPy array.\nWe can index elements in an array using square brackets and indices, just like as with lists. In NumPy indexing starts at 0, just like with a Python list.\n\nv = np.array([12,4,1,9])\n\n# Element in position 0\nprint(v[0])\n\n# Element in position 2\nprint(v[2])\n\n# Element in position -1 (last element)\nprint(v[-1]) # Same as v[3]\n\n# Element in position -3 (counted backwards)\nprint(v[-3]) # Same as v[1]\n\n12\n1\n9\n4\n\n\n\n3.3.1 Basic indexing\nIf you want to access the element at position (i,j) from a two-dimensional array, you can use the double bracket notation [i][j], but with arrays you can also use the more compact syntax [i,j].\n\nM = np.array([[10,2,6,7], [-15,6,7,-8], [9,10,11,12],[3,10,6,1]])\n\n# Element at position (1,1)\nprint('List syntax:',M[1][1])\n\n# Element at position (1,1)\nprint('Array syntax', M[1,1])\n\nList syntax: 6\nArray syntax 6\n\n\nIf you want to access row i you can use M[i] or M[i,:].\n\nprint(M[2]) # Gives last row\n\nprint(M[2,:]) # Gives last row\n\n[ 9 10 11 12]\n[ 9 10 11 12]\n\n\nIf you want to access column j you can use M[:,j]. Both here and in the previous command, the colon : is used to indicate that we want all the elements in the respective dimension. So M[:,j] should be interpreted as, we want the elements from all rows in the j-th column.\n\n\n3.3.2 Index slicing\nIndex slicing is the technical name for the index syntax that returns a slice, a consecutive part of an array.\n\nv = np.array([12,4,1,9,11,14,17,98])\n\nprint(v)\n\n[12  4  1  9 11 14 17 98]\n\n\nv[lower:upper]: This return the elements in v at positions lower, lower+1,...,upper-1. Note that the element at position upper is not included.\n\n# Returns v[1], v[2], v[3], v[4], v[5]\nprint(v[1:6]) \n\n[ 4  1  9 11 14]\n\n\nYou can also omit the lower or upper value, in which case it is set to be position 0 or the last position -1, respectively.\n\n# Returns v[3],...,v[8]\nprint(v[3:]) \n\n# Returns v[0],...,v[4]\nprint(v[:5]) \n\n[ 9 11 14 17 98]\n[12  4  1  9 11]\n\n\nv[lower:upper:step]: This returns the elements in v at position lower,lower+step,lower+2*step,...(upper-1)-step, (upper-1). It does the same as [lower:upper], but now in steps of size step.\n\nv = np.array([12,4,1,9,11,14,17,98])\n\n# Returns v[1], v[3], v[5]\nprint(v[1:6:2]) \n\n[ 4  9 14]\n\n\nYou can omit any of the three parameters lower,upper and step\n\n# lower, upper, step all take the default values\nprint(v[::])\n\n# Index in step is 2 with lower and upper defaults \nprint(v[::2]) \n\n# Index in steps of size 2 starting at position 3\nprint(v[3::2]) \n\n[12  4  1  9 11 14 17 98]\n[12  1 11 17]\n[ 9 14 98]\n\n\nYou can also use slicing with negative index values.\n\n# The last three elements of v\nprint(v[-3:]) \n\n[14 17 98]\n\n\nFurthermore, the same principles apply to two-dimensional arrays, where you can specify the desired indices for both dimensions\n\nM = np.array([[10,2,6,7], [-15,6,7,-8], [9,10,11,12],[3,10,6,1]])\n\nprint(M)\n\n[[ 10   2   6   7]\n [-15   6   7  -8]\n [  9  10  11  12]\n [  3  10   6   1]]\n\n\n[a:b, c:d]: This returns the submatrix consisting of the rows a,a+1,...,b-1 and rows c,c+1,...,d. You can also combine this with a step argument, i.e., use [a:b:step1, c:d:step2].\n\n# Returns elements in submatrix formed by rows 2,3 (excluding 4)\n# and columns 1,2 (excluding 3)\nprint(M[2:4,1:3]) \n\n[[10 11]\n [10  6]]\n\n\nIf you want to obtain a submatrix whose rows and/or columns do not form a consecutive range, or if you want to specify these list manually, you can use the ix_() function from NumPy. Its arguments should be a list of row indices, and a list of column indices specifying the indices of the desired submatrix.\n\ni = [0,2,3]\nj = [0,3]\n\n# Returns submatrix formed by rows 0,2,3 and columns 0,3\nprint(M[np.ix_(i,j)])\n\n[[10  7]\n [ 9 12]\n [ 3  1]]\n\n\n\n\n3.3.3 Fancy indexing\nFancy indexing is the name for when an array or list is used instead of indices, to access part of an array. For example, if you want to access elements in the locations (0,3), (1,2) and (1,3), you can define a list of row indices [0,1,1] and columns indices [3,2,3] and access the matrix with these lists.\n\ni = [0,1,1]\nj = [3,2,3]\n\n# Returns M[0,3] = 7, M[1,2] = 7, M[1,3] = -8\nprint(M[i,j])\n\n[ 7  7 -8]\n\n\nAnother way of fance indexing is by using a Boolean list, that indicates for every element whether it should be index (True) or not (False). Such a list is sometimes called a mask.\n\nv = np.array([1,6,2,3,9,3,6])\n\n# Tell for every element whether is should be index\nmask = [False, True, True, True, False, True, False]\n\nprint(v[mask])\n\n[6 2 3 3]\n\n\nTypically, the mask is generated from a Boolean statement. For example, suppose we want to select all elements strictly smaller than 3 and greater or equal than 7 from the array v.\nThe following statements achieve this. Recall that you can use & if you want the first AND the second statement to be satisfied, and | if either the first OR the second has to be satisfied (or both).\n\nmask_37 = (v &lt; 3) | (v &gt;= 7)\n\n# Boolean vector indiciating for ever element in v\n# whether the conditions v &lt; 3 and v &gt;= 7 are satisfied\nprint(mask_37)\n\n[ True False  True False  True False False]\n\n\nWe can now access the elements satisfying these conditions by indexing v with this mask\n\nprint(v[mask_37])\n\n[1 2 9]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#modifying",
    "href": "03-numpy-arrays.html#modifying",
    "title": "3  NumPy arrays",
    "section": "3.4 Modifying",
    "text": "3.4 Modifying\n\n3.4.1 Elements, rows or columns\nUsing similar ways of indexing as in the previous section, we can also modify the elements of an array\n\nM = np.array([[1,1,1,1], [2,2,2,2], [3,3,3,3],[4,4,4,4]])\n\nprint(M)\n\n[[1 1 1 1]\n [2 2 2 2]\n [3 3 3 3]\n [4 4 4 4]]\n\n\n\n# Modify individual element\nM[0,1] = -1\n\nprint(M)\n\n[[ 1 -1  1  1]\n [ 2  2  2  2]\n [ 3  3  3  3]\n [ 4  4  4  4]]\n\n\n\n# Modify (part of a) row\nM[1,[1,2,3]] = [-2,-2,-2]\n\nprint(M)\n\n[[ 1 -1  1  1]\n [ 2 -2 -2 -2]\n [ 3  3  3  3]\n [ 4  4  4  4]]\n\n\n\n# Modify third column to ones\nM[:,3] = np.ones(4)\n\nprint(M)\n\n[[ 1 -1  1  1]\n [ 2 -2 -2  1]\n [ 3  3  3  1]\n [ 4  4  4  1]]\n\n\n\n\n3.4.2 Broadcasting\nThere does not necessarily have to be a match between the part of the matrix that we index, and the dimensions of the data that we want to overwrite that part with.\n\nM = np.array([[1,1,1,1], [2,2,2,2], [3,3,3,3],[4,4,4,4]])\n\nprint(M)\n\n[[1 1 1 1]\n [2 2 2 2]\n [3 3 3 3]\n [4 4 4 4]]\n\n\nFor example, in order to replace the third column of M by ones, we can also do the command below, instead of using np.ones(4).\n\n# Modify third column to ones\nM[:,3] = 1\n\nprint(M)\n\n[[1 1 1 1]\n [2 2 2 1]\n [3 3 3 1]\n [4 4 4 1]]\n\n\nAlthough there is a mismatch between the indexed part on the left (a column) and the data on the right (single number), Python broadcasts the data to an appopriate format by copying it to the correct size. That is, it copies the 1 to an array [1,1,1,1] of ones, which it then places in the third column.\nThis works similar in higher dimensions. Suppose we want to overwrite the second and third row with [1,6,2,3]. Then the indexed part is a 2 \\times 4 array, but the data a 1 \\times 4 array.\n\n# Modify second and third row\nM[2:4,:] = [1,6,2,3]\n\nprint(M)\n\n[[1 1 1 1]\n [2 2 2 1]\n [1 6 2 3]\n [1 6 2 3]]\n\n\nPython here first copies the data to [[1,6,2,3],[1,6,2,3]] and then modifies M with this array.\n\n\n3.4.3 Transpose\nAnother useful function, in the context of linear algebra, is to take the transpose of a two-dimensional array M, which modifies the entries along the diagonal.\n\nM = np.array([[1,2,3],[3,4,-1]])\n\nprint(M)\n\n[[ 1  2  3]\n [ 3  4 -1]]\n\n\n\ntranspose_M = M.T #np.transpose(M) also works\nprint(transpose_M) \n\n[[ 1  3]\n [ 2  4]\n [ 3 -1]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#repeating-and-stacking",
    "href": "03-numpy-arrays.html#repeating-and-stacking",
    "title": "3  NumPy arrays",
    "section": "3.5 Repeating and stacking",
    "text": "3.5 Repeating and stacking\nWe can also use existing matrices and build new ones from it by stacking them either horizontally or vertically.\ntile(M,(k,r)): This function takes an array M and copies it k times vertically and r times horizontally, resulting in a “tiling” of the original array M.\n\nM = np.array([[1,2],[3,4]])\n\nM_tile = np.tile(M,(2,3))\nprint(M_tile)\n\n[[1 2 1 2 1 2]\n [3 4 3 4 3 4]\n [1 2 1 2 1 2]\n [3 4 3 4 3 4]]\n\n\nIf you do not input a tuples with two arguments, but only a number, then tile() does the tiling only horizontally.\n\nM = np.array([[1,2],[3,4]])\n\nM_tile = np.tile(M,4)\nprint(M_tile)\n\n[[1 2 1 2 1 2 1 2]\n [3 4 3 4 3 4 3 4]]\n\n\nrepeat(M,k): This function takes every element of M, repeats it k times, and puts all these numbers in a one-dimension array.\n\nM = np.array([[1,2],[3,4]])\n\nM_repeat = np.repeat(M,3)\nprint(M_repeat)\n\n[1 1 1 2 2 2 3 3 3 4 4 4]\n\n\nvstack((a,b)): This stacks two arrays a and b vertically, provided they have the correct dimensions to do this. Note that a and b should be inputted as a tuple (a,b).\n\na = np.array([7,8])\nM = np.array([[1,2],[3,4]])\n\nM_a = np.vstack((M,a))\nprint(M_a)\n\n[[1 2]\n [3 4]\n [7 8]]\n\n\nhstack((a,b)): This stacks two arrays a and b horizontally, provided they have the correct dimensions to do this.\nNote that in the example below we define a as a 1 \\times 2 array, i.e., a column array, to make sure we can stack it right of M. If we would have kept a = np.array([7,8]) then Python will give an error, because it cannot stack a row vector next to a two-dimensional array.\n\na = np.array([[7],[8]])\nM = np.array([[1,2],[3,4]])\n\nM_a = np.hstack((M,a))\nprint(M_a)\n\n[[1 2 7]\n [3 4 8]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#reshaping",
    "href": "03-numpy-arrays.html#reshaping",
    "title": "3  NumPy arrays",
    "section": "3.6 Reshaping",
    "text": "3.6 Reshaping\nIt is possible to adjust the shape of an array, while keeping the data of the array the same. For example, consider the x = [1,2,3,\\dots,12].\n\nx = np.arange(1,13)\n\nprint(x)\n\n[ 1  2  3  4  5  6  7  8  9 10 11 12]\n\n\nWe can reshape it into the 3 \\times 4 matrix \nM = \\left[ \\begin{matrix} 1 & 2 & 3 & 4\\\\ 5 & 6 &7 & 8 \\\\ 11 & 10 & 11 & 12 \\end{matrix}\\right]\n by using the reshape(a,b) method. It reshapes x to an a \\times b array provided that a \\cdot b equal the size (i.e., number of elements) of x.\n\n# Reshape x to a 3-by-4 matrix\nM = x.reshape(3,4)\n\nprint(M)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nWe can also reshape two-dimensional arrays, for example, we can reshape M again to a 2 \\times 6 matrix.\n\n# Reshape M to a 2-by-6 matrix\nN = M.reshape(2,6)\n\nprint(N)\n\n[[ 1  2  3  4  5  6]\n [ 7  8  9 10 11 12]]\n\n\nYou should observe that Python does the reshaping in a very specific way: When we transform x to M above, Python fills the matrix M in a row-by-row fashion (instead of column-by-column). This is because of what is called the largest (axis) index changest fastest principle.\nTo understand this idea, recall that we can access the element at position (i,j) of a matrix M with M[i,j]. Here i is the row-index at position 0 of the index list [i,j], and j is the column index at position 1 of the index list [i,j]. We said that the row indices form the 0-axis of the matrix, and the column indices the 1-axis.\n\n\n\nAxes of a two-dimensional array\n\n\nLargest (axis) index changing fastest means that an m \\times n matrix gets filled first along the 1-axis, i.e., it fills the positions (0,0), (0,1), ..., (0,n) while keeping the row index 0 fixed. It then moves up one row index, i.e., one position along the 0-axis and fills the elements (1,0),(1,1),..., (1,n), i.e., the elements along the 1-axis. It continues in this fashion until the complete matrix is full.\nAnother convenient method for reshaping is flatten(), which turns a matrix of any size into a one-dimensional array.\n\n# Define 2-by-3 matrix\nM = np.array([[9,1,3],[2,4,3]])\n\n# Turn into one-dimensional array\nx = M.flatten()\nprint(x)\n\n[9 1 3 2 4 3]\n\n\nIf you want to turn a one-dimensional array x = [x_0,\\dots,x_{n-1}] into a column array of shape (n,1), you can do this as follows.\n\nx = np.array([1,2,4,3,8])\nn = np.size(x)\n\nx = x.reshape(n,1)\nprint(x)\n\n[[1]\n [2]\n [4]\n [3]\n [8]]\n\n\nA more direct way of doing this, is by using x[:,None].\n\nx = np.array([1,2,4,3,8])\nx = x[:,None] # Turns x into column array of shape (n,1)\n\nprint(x)\n\n[[1]\n [2]\n [4]\n [3]\n [8]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "03-numpy-arrays.html#copy-vs.-view",
    "href": "03-numpy-arrays.html#copy-vs.-view",
    "title": "3  NumPy arrays",
    "section": "3.7 Copy vs. view",
    "text": "3.7 Copy vs. view\nIn the last sections we have seen various ways of using arrays to create other arrays. One point of caution here is whether or not the new array is a view or a copy of the original array.\n\n3.7.1 View\nA view y of an array x is another array that simply displays the elements of the array x in a different array, but the elements will always be the same. This means that if we would change an element in the array x, the same element will change in y and vice versa.\n\nx = np.array([[4,2,6],[7,11,0]])\ny = x # This create a view of x\n\nprint('y = \\n', y)\n\ny = \n [[ 4  2  6]\n [ 7 11  0]]\n\n\nWe next change an element in x. Note that the same element changes in y.\n\n# Change element in x\nx[0,2] = -30\n\n# y now also changes in that position\nprint('y = \\n',y)\n\ny = \n [[  4   2 -30]\n [  7  11   0]]\n\n\nThe same happens the other way around: If we change an element in y, then the corresponding element in x also changes.\n\n# Change element in y\ny[1,1] = 100\n\n# x now also changes in that position\nprint('x = \\n', x)\n\nx = \n [[  4   2 -30]\n [  7 100   0]]\n\n\nNote that the same behaviour occurs in we apply the reshape() method.\n\n# Define x = [1,2,...,12]\nx = np.arange(1,13)\n\n# Reshape x to a 3-by-4 matrix\nM = x.reshape(3,4) # Creates view of x\n\nprint(M)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nIf we now change an element in M, then the corresponding element changes in x. This mean that M is a view of the original array x.\n\n# Change element in M\nM[1,3] = 50\n\n# x now also changes in that position\nprint(x)\n\n[ 1  2  3  4  5  6  7 50  9 10 11 12]\n\n\n\n\n3.7.2 Copy\nA copy of an array x is an array z that is completely new and independent of x, meaning that if we change an element in x, then the corresponding element in z does not change, and vice versa. To obtain a copy of x, we can simply apply the copy() method to it.\n\n# Define x = [1,2,...,12]\nx = np.arange(1,13)\n\nz = x.copy() # Create copy of x\nz[0] = -10 # Change element of z\n\nprint('z = \\n', z)\nprint('x = \\n', x) # x has not changed\n\nz = \n [-10   2   3   4   5   6   7   8   9  10  11  12]\nx = \n [ 1  2  3  4  5  6  7  8  9 10 11 12]\n\n\nNote that in the above example, x remains unchanged when we modify the element of z at position 0.\nSimilarly, to turn a reshaped array into a copy, we can apply the copy() method to it.\n\n# Define x = [1,2,...,12]\nx = np.arange(1,13)\n\n# Reshape x to a 3-by-4 matrix\nM = x.reshape(3,4).copy() # Create copy \nM[0,0] = -10 # Change element of x\n\nprint('M = \\n', M)\nprint('x = \\n', x) # x has not changed\n\nM = \n [[-10   2   3   4]\n [  5   6   7   8]\n [  9  10  11  12]]\nx = \n [ 1  2  3  4  5  6  7  8  9 10 11 12]\n\n\nThe flatten() method actually directly creates a copy of the original array.\n\n# Define 2-by-3 matrix\nM = np.array([[9,1,3],[2,4,3]])\n\n# Turn into one-dimensional array\nx = M.flatten() # Creates copy of M\nx[0] = 100 # Change element in x\n\nprint('x = \\n', x) \nprint('M = \\n', M) # M has not changed\n\nx = \n [100   1   3   2   4   3]\nM = \n [[9 1 3]\n [2 4 3]]\n\n\n\nIt is important to know whether a Python function or command creates a copy or a view of the original array. You can typically look this up in the documentation of Python. Otherwise, experiment with the function or command to be sure how it behaves.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy arrays</span>"
    ]
  },
  {
    "objectID": "04-vectorization.html",
    "href": "04-vectorization.html",
    "title": "4  Vectorization",
    "section": "",
    "text": "4.1 Arithmetic operations\nAll basic arithmetic operations (addition, substraction, division, multiplication and comparison) are vectorized in Python. We will illustrate this with the addition operation +, but the same commands can be applied to the other arithmetic operations -, /, *, and &gt;=, ==, &lt;=, !=.\nThe addition operation + can be used to add two numbers together, as you well know. It can also add two arrays, i.e., it works as well for one- and two-dimensional arrays if they have the same shape. This is the usual addition operation you learn about when studying linear algebra.\nx = np.array([1,4,7])\ny = np.array([2,4,3])\n\nprint('x + y =\\n',x+y)\n\nx + y =\n [ 3  8 10]\nA = np.array([[1,2],[1,4]])\nN = np.array([[7,9],[3,4]])\n\nprint('A + N = \\n',A+N)\n\nA + N = \n [[ 8 11]\n [ 4  8]]\nPython is also able to handle addition of arrays of different shapes in certain cases, using the concept of broadcasting that we have seen before. For example, we can add a single number to any array, in which case Python adds this number to every element in the array. This can be seen as an instance of vectorization.\nc = 5\n\nprint('A + c =\\n', A+c)\n\nA + c =\n [[6 7]\n [6 9]]\nWe can also add either a one-dimensional array x of size n to an m \\times n matrix A. In this case, the array x gets added to every row of A:\n\\begin{align*}\nA + x  = &\n\\left[\n\\begin{matrix}\na_{00} & a_{01} & \\dots & a_{0(n-1)} \\\\\na_{10} & a_{11} & \\dots & a_{1(n-1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{(m-1)0} & a_{(m-1)1} & \\dots & a_{(m-1)(n-1)}\n\\end{matrix}\n\\right] +\n\\left[\n\\begin{matrix}\nx_{0} & x_{1} & \\dots & x_{(n-1)}\n\\end{matrix}\n\\right] \\\\\n= &\n\\left[\n\\begin{matrix}\na_{00} + x_0 & a_{01} + x_1 & \\dots & a_{0(n-1)} + x_{n-1} \\\\\na_{10} + x_0 & a_{11} + x_1 & \\dots & a_{1(n-1)} + x_{n-1}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{(m-1)0} + x_0 & a_{m1} + x_1 & \\dots & a_{(m-1)(n-1)} + x_{n-1}\n\\end{matrix}\n\\right]\n\\end{align*}\nIn the example below, we have m = 3 and n = 2.\nx = np.array([10,12])\nprint('Shape of x:', np.shape(x))\n\nA = np.array([[1,2],[1,4],[3,1]])\nprint('Shape of A:', np.shape(A))\n\nprint('A + x = \\n', A + x)\n\nShape of x: (2,)\nShape of A: (3, 2)\nA + x = \n [[11 14]\n [11 16]\n [13 13]]\nAgain, this can be seen as an instance of vectorization, since Python automatically adds x to every row of the matrix A.\nAddition works in the same way if we define x explicitly as an array of shape (1,n).\nx = np.array([[10,12]])\nprint('Shape of x:', np.shape(x))\n\nA = np.array([[1,2],[1,4],[3,1]])\nprint('Shape of A:', np.shape(A))\n\nprint('A + x =\\n', A + x)\n\nShape of x: (1, 2)\nShape of A: (3, 2)\nA + x =\n [[11 14]\n [11 16]\n [13 13]]\nThe same works if we define x = [x_0,\\dots,x_{m-1}]^T as a column array of shape (m,1), in which case it gets added to every column of the matrix A of shape (m,n):\n\\begin{align*}\nA + x  = &  \\left[\n\\begin{matrix}\na_{00}  & a_{01} & \\dots & a_{0(n-1)} \\\\\na_{10}  & a_{11} & \\dots & a_{1(n-1)} \\\\\n\\vdots  & \\vdots & \\ddots & \\vdots \\\\\na_{(m-1)0}  & a_{(m-1)1}  & \\dots & a_{(m-1)(n-1)}\n\\end{matrix}\n\\right]+\n\\left[\n\\begin{matrix}\nx_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{(m-1)} \\\\\n\\end{matrix}\n\\right] \\\\\n= &\n\\left[\n\\begin{matrix}\na_{00} + x_0 & a_{01} + x_0& \\dots & a_{0(n-1)} + x_0\\\\\na_{10} + x_1 & a_{11} + x_1& \\dots & a_{1(n-1)} + x_1\\\\\n\\vdots  & \\vdots & \\ddots & \\vdots\\\\\na_{(m-1)0} + x_{m-1} & a_{(m-1)1} + x_{m-1} & \\dots & a_{(m-1)(n-1)} + x_{m-1}\n\\end{matrix}\n\\right]\n\\end{align*}\nx = np.array([[10],[12],[14]])\nprint('Shape of x:', np.shape(x))\n\nA = np.array([[1,2],[1,4],[3,1]])\nprint('Shape of A:', np.shape(A))\n\nprint('A + x =\\n', A + x)\n\nShape of x: (3, 1)\nShape of A: (3, 2)\nA + x =\n [[11 12]\n [13 16]\n [17 15]]\nWe cannot add arrays of any dimensions to each other. For example, if we would try to add a a 2 \\times 2 array to a 4 \\times 2 array, then Python will return ValueError: operands could not be broadcast together with shapes (4,2) (2,2), i.e., Python cannot perform this addition.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectorization</span>"
    ]
  },
  {
    "objectID": "04-vectorization.html#arithmetic-operations",
    "href": "04-vectorization.html#arithmetic-operations",
    "title": "4  Vectorization",
    "section": "",
    "text": "Note that the shape of x is (n,) which is the syntax that Python uses to denote that x only has one dimension.  You can define x = [x_0,\\dots,x_{n-1}] explicitly as a row vector of shape (1,n) by defining x = np.array([[x_0,...,x_{n-1}]]), that is, with double brackets. It is sometimes needed to change the shape of an array from (n,) to (1,n) or (n,1) to be able to use a function from NumPy.\n\n\n\n\n\n\n\n\n4.1.1 Multiplication broadcasting\nWe emphasize that the broadcasting concepts above also apply to the multiplication operator *. That is, if x is a column array then A*x multiplies every column of A in a pointwise fashion with the array x. Similarly, for two matrix A and B, the syntax A*B returns a matrix in which all elements of A and B are pointwise multiplied with each other, that is, entry (i,j) contains a_{ij}\\cdot b_{ij}.\nThis is not the same as, e.g., the matrix-vector multiplication Ax in the linear algebra sense, i.e,\n\\begin{align*}\nAx = &  \\left[\n\\begin{matrix}\na_{00}  & a_{01} & \\dots & a_{0(n-1)} \\\\\na_{10}  & a_{11} & \\dots & a_{1(n-1)} \\\\\n\\vdots & \\vdots& \\ddots  & \\vdots\\\\\na_{(m-1)0}  & a_{(m-1)1}  & \\dots & a_{(m-1)(n-1)}\n\\end{matrix}\n\\right]\\left[\n\\begin{matrix}\nx_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{(m-1)} \\\\\n\\end{matrix}\n\\right] \\\\\n= &\n\\left[\n\\begin{matrix}\na_{00}x_0 + a_{01}x_1 + \\dots + a_{0(n-1)}x_{n-1}\\\\\na_{10}x_0 + a_{11}x_1 + \\dots + a_{1(n-1)}x_{n-1}\\\\\n\\vdots  \\\\\na_{(m-1)0}x_0 + a_{(m-1)1}x_1 + \\dots + a_{(m-1)(n-1)}x_{n-1}\\\\\n\\end{matrix}\n\\right]\n\\end{align*}\nWe will see matrix-vector and matrix-matrix multiplications in the linear algebra sense later in this book.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectorization</span>"
    ]
  },
  {
    "objectID": "04-vectorization.html#mathematical-functions",
    "href": "04-vectorization.html#mathematical-functions",
    "title": "4  Vectorization",
    "section": "4.2 Mathematical functions",
    "text": "4.2 Mathematical functions\nMany mathematical functions in NumPy are also vectorized by default. Here you should think of functions like\n\nTrigonometry: sin(), cos(), tan()\nExponentation and logarithms: exp(), log(), log10(), log2()\nRounding: around(), floor(), ceil()\nDivision with remainder: mod(), divmod()\nPower computation: sqrt(), abs(), power()\n\nYou access them using np.function_name(). Let us look at some examples; you can check out the documentation of the other functions yourself.\n\nx = np.array([2,1,6])\n\n# Compute sin(i) for every element i in x\ny = np.sin(x)\nprint(y)\n\n[ 0.90929743  0.84147098 -0.2794155 ]\n\n\n\nA = np.array([[2,1,6],[1,1,3]])\n\n# Compute e^i for every element i in A\ny = np.exp(A)\nprint(y)\n\n[[  7.3890561    2.71828183 403.42879349]\n [  2.71828183   2.71828183  20.08553692]]\n\n\n\nx = np.array([1.249583, 3.110294, 4.51139])\n\n# Round every number in x to two decimals\nx = np.around(x, decimals=2)\nprint(x)\n\n[1.25 3.11 4.51]\n\n\n\nx = np.array([10,9,4]) \ny = np.array([2,4,5])\n\n# Compute x_i (mod y_i) for all i\n# and output divisor and remainder\nz = np.divmod(x,y)\nprint(z)\n\n(array([5, 2, 0]), array([0, 1, 4]))\n\n\nnp.divmod() outputs two arrays: the divisors and the remainder. For example, looking at x[1] = 9 and y[1] = 4: the number 4 fits twice in 9, after which 1 is left, i.e., 9 = 2 \\cdot 4 + 1. The number 2 appears in the second position of the first array, and the remainder 1 in the second position of the second array.\n\nA = np.array([[2,3,6],[4,2,3]])\nN = np.array([[1,2,3],[1,2,3]])\n\n# Pointwise compute a_{ij}^n_{ij} for all i,j\nP = np.power(A,N)\nprint(P)\n\n[[  2   9 216]\n [  4   4  27]]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectorization</span>"
    ]
  },
  {
    "objectID": "04-vectorization.html#operations-along-array-axes",
    "href": "04-vectorization.html#operations-along-array-axes",
    "title": "4  Vectorization",
    "section": "4.3 Operations along array axes",
    "text": "4.3 Operations along array axes\nAnother efficient way to perform vectorized operations is to exploit the fact that many NumPy functions that perform an operation on a one-dimensional array, can also be used for two-dimensional arrays where the operation is then either performed on every column (i.e., along the 0-axis), or on every row (i.e., along the 1-axis).\n\n\n\nAxes of a two-dimensional array\n\n\nWe will look at some examples of this in the next sections.\n\n4.3.1 Sorting and searching\nThe function sort() can be used to sort the elements in a one-dimensional array in ascending order, i.e., smallest to largest.\n\n\nx = np.array([0.89, 0.5,  0.57, 0.34])\n\n# Sort and print the elements in x\nx_ascending = np.sort(x)\nprint(x_ascending)\n\n[0.34 0.5  0.57 0.89]\n\n\nIt is not possible to use sort() to sort in descending order, i.e., largest to smallest, but this can be accomplished by reversing the sorted array. We can do this using index slicing with a step size of -1, starting at position -1, meaning that Python goes backwards through the array.\n\n# Access x from beginning till end with step size -1\nx_descending = x_ascending[-1::-1] \n\nprint(x_descending)\n\n[0.89 0.57 0.5  0.34]\n\n\nVectorizing the (ascending) sort operation means we want to have a function that can take as input a two-dimensional array, and return for every row (or column) the sorted list of numbers. It turns out that sort() can do this right away, by adding an additional keyword argument axis.\nAdding axis=0 means that Python will sort the numbers in every column, i.e., along the 0-axis, and axis=1 will sort numbers in every row, i.e., along the 1-axis.\n\nA = np.array([\n [0.89, 0.5,  0.57, 0.34],\n [0.61, 0.12, 0.04, 1.  ],\n [0.27, 0.26, 0.28, 0.25],\n [0.9,  0.84, 0.15, 1.  ]])\n\n# Sort elements in every column\nA_col_ordered = np.sort(A,axis=0)\nprint(A_col_ordered)\n\n[[0.27 0.12 0.04 0.25]\n [0.61 0.26 0.15 0.34]\n [0.89 0.5  0.28 1.  ]\n [0.9  0.84 0.57 1.  ]]\n\n\n\n# Sort elements in every row\nA_row_ordered = np.sort(A,axis=1)\nprint(A_row_ordered)\n\n[[0.34 0.5  0.57 0.89]\n [0.04 0.12 0.61 1.  ]\n [0.25 0.26 0.27 0.28]\n [0.15 0.84 0.9  1.  ]]\n\n\nWe remark that sort() creates a copy, and not a view of the original matrix (see Chapter 3.7).\nAnother useful sorting function is argsort() that, for a given array x = [x_0,\\dots,x_{n-1}] outputs an array whose i-th element is the position of the number in x that appears in place i in the ordered array np.sort(x).\n\nx = np.array([0.89, 0.5,  0.57, 0.34])\n\n# Position in original array or elements in ordered list\norder = np.argsort(x) # np.sort(x) = [0.34, 0.5, 0.57, 0.89]\nprint(order)\n\n# Obtaining sort() from argsort()\nprint(x[order])\n\n[3 1 2 0]\n[0.34 0.5  0.57 0.89]\n\n\nIn the example above, in the ordered list np.sort(x) the first number is 0.34, which appears at position 3 in x; the second number is 0.5, which appears at position 1 in x, the third number is 0.57 which appears in position 2 in x; and the fourth number is 0.89, which appears in position 0 in x.\nThis function also works for two-dimensional arrays. For example, determining the relative order of the elements in every column can be done by adding axis=0 (and similarly in every row by using axis=1).\n\nA = np.array([\n [0.89, 0.5,  0.57, 0.34],\n [0.61, 0.12, 0.04, 1.  ],\n [0.27, 0.26, 0.28, 0.25],\n [0.9,  0.84, 0.15, 1.  ]])\n\n# Determine relative order in every column\nN = np.argsort(A,axis=0)\nprint(N)\n\n[[2 1 1 2]\n [1 2 3 0]\n [0 0 2 1]\n [3 3 0 3]]\n\n\n\n\n4.3.2 Summary statistics\nThere are various other mathematical functions that can perform operations along axes by adding the axis keyword argument. Here we list some common ones from NumPy, that yield so-called summary statistics of a (one-dimensional) array:\n\nSum and product: sum(), prod(),\nMean, standard deviation, median: mean(), std(), median()\nMaximum and minimum: max(), min().\n\nWe will illustrate the use of these six functions using max(), but the same code applies to all other functions (if the task at hand is mathematically well-defined).\n\nA = np.array([\n[2,3,6],\n[4,2,3]\n])\n\nIf we apply the max() function directly to a (two-dimensional) array, it will give the maximum value in the whole array.\n\n# Gives maximum of all elements in A\nA_max = np.max(A)\n\nprint(A_max)\n\n6\n\n\nIf we add the axis keyword argument, we can either obtain the maximum of every row, or every column.\n\n# Gives maximum of every column\nA_column_max = np.max(A,axis=0)\n\nprint(A_column_max)\n\n[4 3 6]\n\n\n\n# Gives max of every row\nA_row_max = np.max(A,axis=1)\n\nprint(A_row_max)\n\n[6 4]\n\n\nAnother useful function is argmax() than can return the index (position) at which the maximum in an array is attained.\n\n# Gives position of maximum in every column\nA_col_argmax = np.argmax(A,axis=0)\n\nprint(A_col_argmax)\n\n[1 0 0]\n\n\n\n# Gives position of maximum in every row\nA_row_argmax = np.argmax(A,axis=1)\n\nprint(A_row_argmax)\n\n[2 0]\n\n\nNote that the array containing the positions of the maxima is given as a row array. If you want to turn this into a column array (because the rows are ordered vertically in a two-dimensional array), recall you can do this as follows.\n\nA_row_argmax = A_row_argmax[:,None]\n\nprint(A_row_argmax)\n\n[[2]\n [0]]\n\n\nIf we try np.argmax(A) without using the axis keyword argument, then Python first flattens the matrix into a one-dimensional array, after which it returns the position of the maximum in the flattened array. Note that this flattening happens according to the largest index changing fasted principle (so it it places all the rows after each other, and not all the columns under each other).\nAlso note that if the maximum is attained in multiple places, then Python only returns the position of the first element that attains the maximum.\n\n# Gives position of maximum \nN = np.array([\n[2,3,4],\n[4,4,3]\n])\n\nN_argmax = np.argmax(N) # Turns N into [2,3,4,4,4,3];\n                        # returns first position with maximum \n\nprint(N_argmax)\n\n2\n\n\nThere are also more advance functions that give some summative information about an array:\n\nCumulative sum: cumsum(),\nCumulative product: cumprod().\n\nThe function cumsum() return the cumulative sum of a one-dimensional array. As an example, if x = [1,4,2,5], then the cumulative sums will be given by\n\nx_{\\text{cumsum}} = [1, 1 + 4, 1 + 4 + 2, 1 + 4 + 2 + 5] = [1,5,7,12].\n\n\nx = np.array([1,4,2,5])\n\n# Cumulative sum of x\nx_cumsum = np.cumsum(x)\nprint('x_cumsum =', x_cumsum)\n\nx_cumsum = [ 1  5  7 12]\n\n\nThe function can also be vectorized using the axis keyword argument.\nThe function cumprod() returns the cumulative product. Again, if x = [1,4,2,5], then the cumulative products will be given by\n\nx_{\\text{cumprod}} = [1, 1 \\cdot 4, 1 \\cdot 4 \\cdot 2, 1 \\cdot 4 \\cdot 2 \\cdot 5] = [1,4,8,40].\n\n\nx = np.array([1,4,2,5])\n\n# Cumulative product of x\nx_cumprod = np.cumprod(x)\nprint('x_cumprod =', x_cumprod)\n\nx_cumprod = [ 1  4  8 40]\n\n\nThis function can also be vectorized using the axis keyword argument.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectorization</span>"
    ]
  },
  {
    "objectID": "05-linear.html",
    "href": "05-linear.html",
    "title": "5  Linear algebra and optimization",
    "section": "",
    "text": "5.1 Linear algebra\nIn this section we will often refer to one-dimensional arrays as row or column vectors, and to two-dimensional arrays as matrices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "05-linear.html#linear-algebra",
    "href": "05-linear.html#linear-algebra",
    "title": "5  Linear algebra and optimization",
    "section": "",
    "text": "5.1.1 Matrix multiplications\nRecall from Section 4.1.1 that for an m \\times n matrix A and m \\times 1 column vector x, the command A*x gives a matrix in which every column of A gets pointwise multiplied by the column vector x, that is,\n\\begin{align*}\nA*x  = &  \\left[\n\\begin{matrix}\na_{00}  & a_{01} & \\dots & a_{0(n-1)} \\\\\na_{10}  & a_{11} & \\dots & a_{1(n-1)} \\\\\n\\vdots  & \\vdots & \\ddots & \\vdots \\\\\na_{(m-1)0}  & a_{(m-1)1}  & \\dots & a_{(m-1)(n-1)}\n\\end{matrix}\n\\right]*\\left[\n\\begin{matrix}\nx_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{(m-1)} \\\\\n\\end{matrix}\n\\right] \\\\\n= &\n\\left[\n\\begin{matrix}\na_{00} \\cdot x_0 & a_{01} \\cdot  x_0& \\dots & a_{0(n-1)} \\cdot  x_0\\\\\na_{10} \\cdot  x_1 & a_{11} \\cdot  x_1& \\dots & a_{1(n-1)} \\cdot  x_1\\\\\n\\vdots &  \\vdots& \\ddots & \\vdots \\\\\na_{(m-1)0} \\cdot  x_{m-1} & a_{(m-1)1} \\cdot  x_{m-1} & \\dots & a_{(m-1)(n-1)} \\cdot  x_{m-1}\n\\end{matrix}\n\\right]\n\\end{align*}\n\nA = np.array([\n[1,2],\n[3,4]])\n\nx = np.array([1,5])\nx = x[:,None] # Turn x into (2,1) shaped column vector\n\nprint('A = \\n', A)\nprint('x = \\n', x)\nprint('A*x = \\n', A*x)\n\nA = \n [[1 2]\n [3 4]]\nx = \n [[1]\n [5]]\nA*x = \n [[ 1  2]\n [15 20]]\n\n\nIn the linear algebra sense, the multiplication Ax gives an m \\times 1 column vector defined by\n\\begin{align*}\nAx = &  \\left[\n\\begin{matrix}\na_{00}  & a_{01} & \\dots & a_{0(n-1)} \\\\\na_{10}  & a_{11} & \\dots & a_{1(n-1)} \\\\\n\\vdots  & \\vdots & \\ddots & \\vdots\\\\\na_{(m-1)0}  & a_{(m-1)1}  & \\dots & a_{(m-1)(n-1)}\n\\end{matrix}\n\\right]\\left[\n\\begin{matrix}\nx_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{(m-1)} \\\\\n\\end{matrix}\n\\right] \\\\\n= &\n\\left[\n\\begin{matrix}\na_{00}x_0 + a_{01}x_1 + \\dots + a_{0(n-1)}x_{n-1}\\\\\na_{10}x_0 + a_{11}x_1 + \\dots + a_{1(n-1)}x_{n-1}\\\\\n\\vdots  \\\\\na_{(m-1)0}x_0 + a_{(m-1)1}x_1 + \\dots + a_{(m-1)(n-1)}x_{n-1}\\\\\n\\end{matrix}\n\\right]\n\\end{align*}\nThis can be achieved in Python with the @ (‘at’) operator, that is, by writing A @ x (the white spaces are not needed; they are included here for readability).\n\nprint('A = \\n', A)\nprint('x = \\n', x)\n\n\nprint('Ax = \\n', A @ x)\n\nA = \n [[1 2]\n [3 4]]\nx = \n [[1]\n [5]]\nAx = \n [[11]\n [23]]\n\n\nIf we talk about matrix multiplication, it will always be clear from context whether we mean A*x or A @ x.\nWe can also use @ to multiply a k \\times m and m \\times n matrix with each other in the linear algebra sense.\n\nA = np.array([\n[1,2],\n[3,4],\n[3,7]])\n\nB = np.array([\n[2,5,1,7],\n[3,4,-1,8]])\n\n# Note that k = 3, m = 2, n = 4\nprint('AB = \\n',A @ B)\n\nAB = \n [[ 8 13 -1 23]\n [18 31 -1 53]\n [27 43 -4 77]]\n\n\nA special case of this is if we multiple a column vector\n\nx = \\left[\\begin{matrix} x_0\\\\ x_1 \\\\ \\vdots \\\\x_{n-1} \\end{matrix} \\right]\n\nwith its transpose (being a row vector) resulting in a matrix that contains x_ix_j on position (i,j).\n\nx = np.array([\n[1],\n[2],\n[4]])\n\nprint('xx^T = \\n', x @ x.T)\n\nxx^T = \n [[ 1  2  4]\n [ 2  4  8]\n [ 4  8 16]]\n\n\n\n\n5.1.2 Matrix properties\nUsing the linalg subpackage from NumPy, we can compute various well-known properties of a matrix (that you should recall from your Linear Algebra course).\n\nlinalg.matrix_rank(A): Computes rank of matrix A.\nlinalg.det(A): Computes determinant of square A.\nlinalg.eig(A): Computes eigenvalues and (right) eigenvectors of matrix A, i.e., values \\lambda that satisfy Av = \\lambda v for some (eigen)vector v.\nlinalg.inv(A): Computes the inverse matrix A^{-1} of A, that is, the matrix that satisfies A^{-1}A = AA^{-1} = I, where I is the identity matrix.\nlinalg.norm(A): Computes the norm of matrix (or vector).\n\nWe can access these function with the syntac np.linalg.function_name(A).\nFinally, there is also the trace function trace(A) in NumPy that computes the trace of A, i.e., the sum of the elements on the diagonal. This function is implemented directly in NumPy, so you don’t have to go to the subpackage linalg first.\nLet us look at some examples of these commands.\n\nA = np.array([\n[1,2],\n[3,4]])\n\n\n# Rank of A\nprint('Rank of A: ', np.linalg.matrix_rank(A))\n\nRank of A:  2\n\n\n\n# Determinant of A\nprint('Determinant of A: ', np.linalg.det(A))\n\nDeterminant of A:  -2.0000000000000004\n\n\n\n# Trace of A\nprint('Trace of A: ', np.trace(A))\n\nTrace of A:  5\n\n\n\n# Inverse of A\nprint('Inverse of A: \\n', np.linalg.inv(A))\n\nInverse of A: \n [[-2.   1. ]\n [ 1.5 -0.5]]\n\n\nThe function linalg.eig() is special in that it does not output one, but two arrays.\nThe first array is one-dimensional and contains the eigenvalues [\\lambda_0,\\dots,\\lambda_{n-1}] of A; the second array is two-dimensional, and contains the corresponding eigenvectors v^0,\\dots,v^{n-1} as columns of the matrix. The eigenvalues and eigenvectors are paired in the sense that Av^i = \\lambda_iv^i for i = 0,\\dots,n-1.\n\n# Eigenvalues and eigenvectors of A\nlambdas, V = np.linalg.eig(A)\n\nprint('Eigenvalues of A: \\n', lambdas)\nprint('Matrix with eigenvectors of A: \\n', V)\n\nEigenvalues of A: \n [-0.37228132  5.37228132]\nMatrix with eigenvectors of A: \n [[-0.82456484 -0.41597356]\n [ 0.56576746 -0.90937671]]\n\n\nYou can output only the eigenvalues or the eigenvectors by suppressing the other output argment with _.\n\n# Only eigenvalues of A\nlambdas, _ = np.linalg.eig(A)\n\n# Only eigenvectors of A\n_, V = np.linalg.eig(A)\n\nIf you want to recall how you can handle multiple outputs, and suppress the ones that your are not interested, have a look at Appendix B.\nLet us check that the eigenvalues and eigenvectors indeed satsify Av^i = \\lambda_iv^i for i = 0,1.\n\nv0 = V[:,0] \nv1 = V[:,1]\n\nprint('Verify first eigenvector:', A @ v0 - lambdas[0]*v0)\nprint('Verify second eigenvector:', A @ v1 - lambdas[1]*v1)\n\nVerify first eigenvector: [0.00000000e+00 5.55111512e-17]\nVerify second eigenvector: [0. 0.]\n\n\nThe function linalg.norm(A) computes the L^2-norm of a matrix A = (a_{ij})_{i = 1,\\dots,m, j = 1,\\dots,n}, also known as the Frobenius norm. It is defined as\n\n||A||_{\\text{F}} = \\sqrt{\\sum_{i=0}^{m-1} \\sum_{j = 0}^{n-1} a_{ij}^2 }.\n\nIf you apply the function two a row or column vector x = [x_0,\\dots,x_{n-1}], you get the usual L^2-norm defined as\n\n||x||_{2} = \\sqrt{\\sum_{i=0}^{n-1} x_i^2 }.\n\nYou can also use this function in a vectorized manner. For example, if you want to compute the L^2-norm of every row in a matrix, you can use the axis keyword argument.\n\nA = np.array([\n[1,2],\n[3,4],\n[5,6]])\n\nrow_norms = np.linalg.norm(A,axis=1)\n\nprint(row_norms)\n\n[2.23606798 5.         7.81024968]\n\n\n\n\n5.1.3 Equation solving\nFor a given m \\times n matrix A = (a_{ij}) and m \\times 1 column vector b = [b_0,\\dots,b_{m-1}]^T, perhaps the most important question in linear algebra is to compute an x = [x_0,\\dots,x_{n-1}]^T that satisfies Ax = b, i.e., the linear systems of equalities \n\\begin{array}{rrrrcrrrl}\na_{00}x_0 &+& a_{01}x_1 &+& \\dots &+& a_{0(n-1)}x_{n-1} &=& b_0 \\\\\na_{10}x_0 &+& a_{11}x_1 &+& \\dots &+& a_{1(n-1)}x_{n-1} &=& b_1 \\\\\n&&&& \\vdots &&&& \\\\\na_{(m-1)0}x_0 &+& a_{(m-1)1}x_1 &+& \\dots &+& a_{(m-1)(n-1)}x_{n-1} &=& b_{m-1}\n\\end{array}.\n\nIf A is a square, invertible matrix then this can be done with linalg.solve(A,b). This gives the unique solution x = A^{-1}b.\n\nA = np.array([\n[1,2],\n[3,4]])\n\nb = np.array([[1],[4]])\n\nprint('Solution to Ax = b: \\n', np.linalg.solve(A,b))\n\nSolution to Ax = b: \n [[ 2. ]\n [-0.5]]\n\n\nIn fact, you can compute x directly as x = A^{-1}b using the inverse of A that we have seen earlier.\n\nprint('Solution to Ax = b: \\n', np.linalg.inv(A) @ b)\n\nSolution to Ax = b: \n [[ 2. ]\n [-0.5]]\n\n\nIf the system is overdetermined, then there is not necessarily a unique solution. This can happen when there are more constraints than variables, i.e., when m &gt; n. In this case, we can find a solution x that is approximately optimal with the least squares method. It finds a solution x that solves the (non-linear) problem \n\\min_x ||Ax - b||_2.\n The least squares method can be executed with linalg.lstsq(A,b); we will see this method in the next chapter, when we consider non-linear optimization problems. This function can also be used to solve underdetermined systems, where n &gt; m.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "05-linear.html#linear-optimization",
    "href": "05-linear.html#linear-optimization",
    "title": "5  Linear algebra and optimization",
    "section": "5.2 Linear optimization",
    "text": "5.2 Linear optimization\nRecall that in a linear optimization problem the goal is to compute a vector x = [x_0,\\dots,x_{n-1}] of decision variables that maximizes, or minimizes, a linear objective function subject to a collection of linear constraints, which can either be a \\leq, \\geq or = constraint.\n\nIn this section we will consider a general linear optimization problem of the form \n\\begin{array}{ll}\n\\min & c^Tx \\\\\n{\\rm s.t.} & Ax = b \\\\\n& Wx \\leq z \\\\\n& \\ell \\leq x \\leq u\n\\end{array}\n where c, \\ell, u \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, U \\in \\mathbb{R}^{k \\times n}, and z \\in \\mathbb{R}^k are the input data, or input parameters.\nWritten out this means we have a system with m equality constraints, k inequality constraints and upper and lower bounds on the decision variables.\n\n\\begin{array}{rrrcrrlll}\n\\min &  c_1 x_1 &+& \\cdots& + & c_nx_n& & & \\\\\n{\\rm s.t.} &  a_{01}x_0& +& \\cdots& +& a_{0(n-1)} x_{n-1} &= &b_0 &\\\\\n& & & \\vdots & & & & \\\\\n&  a_{(m-1)1}x_0& +& \\cdots& +& a_{(m-1)(n-1)} x_{n-1} &= &b_{m-1}& \\\\\n& w_{01}x_0& +& \\cdots& +& w_{0(n-1)} x_{n-1} &\\leq &z_0 &\\\\\n&  & & \\vdots & & & & &\\\\\n&  w_{(k-1)1}x_0& +& \\cdots& +& w_{(k-1)(n-1)} x_{n-1} &\\leq &z_{k-1} & \\\\\n\\ell_0 \\leq & x_0  &   && &  &  \\leq & u_0&  \\\\\n&   &   & \\ddots & &  &   & &  \\\\\n\\ell_{n-1} \\leq &   &   & & &  x_{n-1} &  \\leq & u_{n-1}&  \\\\\n\\end{array}\n\nYou can model a \\geq constraint by multiplying it with -1, so that it turns into a \\leq constraint.\nIf, in addition, we require that x_i \\in \\mathbb{N} for i = 1,\\dots,n, i.e., that the variables are integral, then we call the above problem an integer linear optimization problem.\nWe will look at two packages that can be used to solve (integer) linear optimization problems. The difference is that the first package linprog allows us to explicitly input the data c, A, B, \\ell and u, whereas the second package pulp allows us to define constraints one-by-one. The latter is more convenient if the input data his not given explicitly.\n\n5.2.1 Explicit input data\nIf the input data is given explicitly, we can solve linear optimization problems quickly with the linprog module, which is part of the optimize package of SciPy. We will see more functionality of SciPy in later chapters of this book.\nWe can import linprog as follows:\n\nfrom scipy.optimize import linprog\n\nThis package is suitable for solving problem of the general form above. We will implement the following example:\n\n\\begin{array}{lrrrrrrl}\n\\text{max } & z  = & 15x_{1} &+ &20x_{2} & & & \\\\\n\\text{s.t.} & & 2x_1 &+ & 2x_2& \\leq & 8\\frac{1}{2} & \\\\\n& & x_1 &+ & 2x_2& \\leq & 6 & \\\\\n&& 12x_1 &+ & 17x_2& = & 51 & \\\\  \n& &x_1 & &  & \\geq & 0 & \\\\\n&  && & x_2 & \\geq & 0 &\n\\end{array}\n\nBecause linprog works with explicitly input data, we first define these. We define all the input data as NumPy arrays, but you can actually also use plain lists for this.\nNote that we multiply the objective function c with -1, so that our maximization problem turns into a minimization problem.\n\n## Define input data\n\n# Coefficients of the objective function\nc = np.array([-15, -20])  # Minimize -15x1 - 20x2\n\n# Coefficients of the equality constraints (Ax = b)\nA = np.array([[12, 17]]) # 12x1 + 17x2 = 51\n\nb = np.array([51])\n\n# Coefficients of the inequality constraints (Ux &lt;= z)\nU = np.array([\n[2, 2],  # 2x1 + 2x2 &lt;= 8.5\n[1, 2]]) # x1 + 2x2 &lt;= 6\n\nz = np.array([[8.5], [6]])\n\nThe bounds on x_i have to be inputted as tuple (\\ell_i,u_i). If the lower or upper bound is not present, meaning either \\ell_i = -\\infty or u_i = + \\infty, you can replace the entry with None.\n\n# Bounds for the variables x1 and x2 (l = 0, u = infinity)\nx1_bounds = (0, None)  # x1 &gt;= 0\nx2_bounds = (0, None)  # x2 &gt;= 0\n\nWe can solve the problem with linprogr() It takes input arguments\n\nc : The vector with objective function coefficients\nA_eq : The constraint matrix for the equality constraints\nb_eq : The right hand side vector of the equality constraints\nA_ub : The constraint matrix for the inequality constraints\nb_ub : The right hand side vector of the inequality constraints\nbounds : List of tuples with each tuple having the upper and lower bound for the corresponding variable\n\nNote that A_eq and b_eq can be left out as input arguments if there are no equality constraints; the same holds for A_ub and b_ub.\n\n# Solve the linear programming problem\n# (You can use \\ to continue command on next line)\nresult = linprog(c, A_eq=A, b_eq= b,  \\\n                    A_ub=U, b_ub=z, \\\n                    bounds=[x1_bounds, x2_bounds])\n\nThe result variable contains various aspects of the solution. The two most important for us are\n\nresult.x : The optimal solution x\nresult.fun : The function value attained by the the optimal solution.\n\n\n# Output the results\nprint('The problem is solved by', result.x, \\\n      'with objective function value', result.fun)\n\nThe problem is solved by [4.25 0.  ] with objective function value -63.75\n\n\nIf we want to solution to be integral, we add the input argument integrality=1. See the documentation to read about this and to find more options.\n\n# Solve the integer linear optimization problem\n# (You can use \\ to continue command on next line)\nresult_integral = linprog(c, A_eq=A, b_eq= b,  \\\n                    A_ub=U, b_ub=z, \\\n                    bounds=[x1_bounds, x2_bounds], \\\n                    integrality=1)\n                    \n# Output the results\nprint('The problem is solved by', result_integral.x, \\\n      'with objective function value', result_integral.fun)\n\nThe problem is solved by [0. 3.] with objective function value -60.0\n\n\nThe complete code of this section can be found below.\n\n\nShow complete code for linear optimization with linprog\nimport numpy as np\nfrom scipy.optimize import linprog\n\n## Define input data\n\n# Coefficients of the objective function\nc = np.array([-15, -20])  # Minimize -15x1 - 20x2\n\n# Coefficients of the equality constraints (Ax = b)\nA = np.array([[12, 17]]) # 12x1 + 17x2 = 51\n\nb = np.array([51])\n\n# Coefficients of the inequality constraints (Ux &lt;= z)\nU = np.array([\n[2, 2],  # 2x1 + 2x2 &lt;= 8.5\n[1, 2]]) # x1 + 2x2 &lt;= 6\n\nz = np.array([[8.5], [6]])\n\n## Bounds for the variables x1 and x2 (l = 0, u = infinity)\nx1_bounds = (0, None)  # x1 &gt;= 0\nx2_bounds = (0, None)  # x2 &gt;= 0\n\n## Solve the linear optimization problem\n## (You can use \\ to continue command on next line)\nresult = linprog(c, A_eq=A, b_eq= b,  \\\n                    A_ub=U, b_ub=z, \\\n                    bounds=[x1_bounds, x2_bounds])\n\n# Output the results\nprint('The linear optimization problem is solved by', result.x, \\\n      'with objective function value', result.fun)\n                    \n##################################                \n## With integrality constraints ##\n##################################  \n\n# Solve the problem\nresult_integral = linprog(c, A_eq=A, b_eq= b,  \\\n                    A_ub=U, b_ub=z, \\\n                    bounds=[x1_bounds, x2_bounds], \\\n                    integrality=1)\n                    \n# Output the results\nprint('The integer linear optimization problem is solved by', result_integral.x, \\\n      'with objective function value', result_integral.fun)\n\n\n\n\n5.2.2 Implicit input data\nIf the constraints are not given explicitly, creating the input data matrices like A and U can be quite tedious. In this case, it works better to define the constraints directly, based on the given problem description.\nIn this section we will consider the maximum weight bipartite matching problem. We are given a (complete) bipartite graph G = (V,W,E) with node sets V = \\{0,\\dots,n-1\\} and W = \\{0,\\dots,n-1\\} and edges ij \\in E = \\{ab : a \\in V, b \\in W\\} connecting the nodes i \\in V with j \\in W. Every such edge has a weight w_{ij}. We want to match up every node in V with a node in W so that the total summed weight of the selected edges is maximal.\nWe introduce (binary) decision variables x_{ij} with the interpretation that x_{ij} = 1 if i and j get matched, and x_{ij} = 0 otherwise. The constraints then mean that every node i \\in V should get matched up with precisely one j \\in W and vice versa.\n\n\\begin{array}{lllll}\n\\max & \\displaystyle \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} w_{ij}x_{ij} & & \\\\\n\\rm{s.t. } & \\displaystyle \\sum_{i=0}^{n-1} x_{ij} & =& 1 & \\text{for } j = 0,\\dots,n-1 \\\\\n&\\displaystyle  \\sum_{j=0}^{n-1} x_{ij} & = &1 & \\text{for } i = 0,\\dots,n-1\\\\\n& x_{ij}  &\\geq & 0 & \\text{for all } i,j = 0,\\dots,n-1 \\\\\n& x_{ij} & \\in & \\{0,1\\} & \\text{for all } i,j = 0,\\dots,n-1\n\\end{array}\n\nIn fact, the last conditions defining the decisions variables to be binary, are redundant: If we leave them out then the optimal solution that is found will satisfy them regardless (you might have seen this in a Combinatorial Optimization course). For now, we will ignore this mathematical fact, and define the decision variables as binary variables.\nDefining the constraint matrix explicitly is quite a hassle, and given that many of its entries are zero, it is not very efficient for Python to store it explicitly, so using linprog here is not very convenient.\nInstead, in this section we will use the pulp package to solve this problem.\n\nimport pulp\n\nWe create some input data (weights) for the problem that we are going to build.\n\n# Size of the node sets\nn = 4\n\n# Weights of the edges ij\nw = np.arange(1,n**2+1).reshape(n,n)\n\nprint(w) \n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]\n [13 14 15 16]]\n\n\nProblem instantiation. We first initialize the problem object that we want to solve with LpProblem() that takes as input:\n\nProblem name: String variable\nProblem type: Maximization (pulp.LpMaximize), or minimization (pulp.LpMinimize).\n\n\nprob = pulp.LpProblem(\"Weighted_Bipartite_Matching\", pulp.LpMaximize)\n\nAs opposed to linprog we need to explicitly instantiate the decision variables, so that we can use them later to add constraints to our problem.\nDecision variables. A decision variable y can be instantiated with LpVariable() that takes as input\n\nVariable name: String being name of variable\nVariable category: Keyword argument cat being Binary, Integer or Continuous (default if cat is not specified).\n\n\npulp.LpVariable('y',cat='Binary')\n\nTo initiate our decision variables we need to loop over the indices i and j, which can be done as follows. Note that the syntax f\"y_{i}_{j}\" allows us to incorporate/format the loop indices i and j into the variable name (which is a string).\n\nx = np.zeros((n,n), dtype=object) # Data type of variable is 'object'\nfor i in range(n):\n    for j in range(n):\n        x[i,j] = pulp.LpVariable(f\"x_{i}_{j}\", cat='Binary')      \n\nThis can be done more compactly using list comprehension, which is a quick alternative for doing the for-loops explicitly. Avoiding for-loops altogether here is not possible, because variables cannot be generated in a vectorized manner if we want to give them all a separate name.\n\n# With list comprehension\nx = np.array([[pulp.LpVariable(f\"x_{i}_{j}\", cat='Binary') \\\n               for j in range(n)] for i in range(n)])\n\nIn both cases we have created an n \\times n NumPy array whose elements are decision variables x_{ij}.\n\n# Print names of decision variables\nprint(x)\n\n[[x_0_0 x_0_1 x_0_2 x_0_3]\n [x_1_0 x_1_1 x_1_2 x_1_3]\n [x_2_0 x_2_1 x_2_2 x_2_3]\n [x_3_0 x_3_1 x_3_2 x_3_3]]\n\n\nWe will use these variables to define the objective function and constraints.\nObjective function. We next construct the objective function \n\\begin{array}{lllll}\n\\max & \\displaystyle \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} w_{ij}x_{ij} & &\n\\end{array}\n Because both the weight matrix w and the decision variables x are stored in NumPy arrays, we can do a pointwise multiplication of these arrays, and sum up the elements of the resulting array, to get the objective.\nSumming up decision variables in arrays, in our case the array w*x, can be done using lpSum() within PuLP. To add the objective to the problem, and later constraints, we use the syntax prob += [objective] where [objective] contains the objective function expresssion in terms of the decision variables that we instantiated. You should think of the syntax += as adding something to the instantiated problem prob.\n\nprob += pulp.lpSum(w*x)  \n\nNote that we write lpSum() and not LpSum() with a capital L. This has to do with the fact that lpSum() is a function, whereas LpProblem and LpVariable are classes (that have different naming conventions in Python modules).\n\n# Print objective function\nprint(prob.objective)\n\nx_0_0 + 2*x_0_1 + 3*x_0_2 + 4*x_0_3 + 5*x_1_0 + 6*x_1_1 + 7*x_1_2 + 8*x_1_3 + 9*x_2_0 + 10*x_2_1 + 11*x_2_2 + 12*x_2_3 + 13*x_3_0 + 14*x_3_1 + 15*x_3_2 + 16*x_3_3\n\n\n\nConstraints. Finally, we add the constraints \n\\begin{array}{lllll}\n& \\displaystyle \\sum_{i=0}^{n-1} x_{ij} & =& 1 & \\text{for } j = 0,\\dots,n-1 \\\\\n&\\displaystyle  \\sum_{j=0}^{n-1} x_{ij} & = &1 & \\text{for } i = 0,\\dots,n-1\n\\end{array}\n to the problem, which can also be done with the prob += [constraint] syntax, where [constraint] is the constraint we want to add. We can add our constraints as follows.\n\nfor j in range(n):\n    prob += pulp.lpSum([x[i,j] for i in range(n)]) == 1\n    \nfor i in range(n):\n    prob += pulp.lpSum([x[i,j] for j in range(n)]) == 1\n\nTo print the constraints, we need to realize that they are stored in an (ordered) dictionary. We can print the keys and values of this dictipnary as follows.\n\n# Print constraints\nfor key, value in prob.constraints.items():\n    print(f\"{key}: {value}\")\n\n_C1: x_0_0 + x_1_0 + x_2_0 + x_3_0 = 1\n_C2: x_0_1 + x_1_1 + x_2_1 + x_3_1 = 1\n_C3: x_0_2 + x_1_2 + x_2_2 + x_3_2 = 1\n_C4: x_0_3 + x_1_3 + x_2_3 + x_3_3 = 1\n_C5: x_0_0 + x_0_1 + x_0_2 + x_0_3 = 1\n_C6: x_1_0 + x_1_1 + x_1_2 + x_1_3 = 1\n_C7: x_2_0 + x_2_1 + x_2_2 + x_2_3 = 1\n_C8: x_3_0 + x_3_1 + x_3_2 + x_3_3 = 1\n\n\n\n Solving the problem. We solve the problem with the solve() function.\nWe can access the values of the objective function and the variables using pulp.value(). The objective function of the problem is stored in prob.objective, and so its value in the optimized model can be accessed with pulp.value(prob.objective).\nWe want to represent the optial matching nicely in a binary matrix. Unfortunately, printing the values of the optimized model is difficult to do without for-loops since the function pulp.value(x[i,j]) that returns the value of variable x_{ij} only works for numbers and not lists or arrays.\n\n# Solve the problem\nprob.solve()\n\n# Store the results in matrix 'matching'\nmatching = np.zeros((n,n), dtype=int)\nfor i in range(n):\n    for j in range(n):\n        matching[i,j] = pulp.value(x[i,j])\n\n# Print solution        \nprint(\"The optimal matching is: \\n\", matching)\nprint(f\"Optimal value of the objective function: {pulp.value(prob.objective)}\")\n\nThe optimal matching is: \n [[1 0 0 0]\n [0 1 0 0]\n [0 0 0 1]\n [0 0 1 0]]\nOptimal value of the objective function: 34.0\n\n\nThe complete code of this section can be found below.\n\n\nShow complete code for linear optimization with PuLp\nimport pulp\n\n# Size of the node sets (input data)\nn = 4\n\n# Weights of the edges ij (input data)\nw = np.arange(1,n**2+1).reshape(n,n)\n\n# Instantiate problem\nprob = pulp.LpProblem(\"Weighted_Bipartite_Matching\", pulp.LpMaximize)\n\n# Instantiate decision variables and store in NumPy array\nx = np.array([[pulp.LpVariable(f\"x_{i}_{j}\", cat='Binary') \\\n               for i in range(n)] for j in range(n)])\n\n# Set objective function\nprob += pulp.lpSum(w*x)  \n\n# Set constraints\nfor j in range(n):\n    prob += pulp.lpSum([x[i,j] for i in range(n)]) == 1\n    \nfor i in range(n):\n    prob += pulp.lpSum([x[i,j] for j in range(n)]) == 1\n    \n# Store the results in matrix 'matching'\nmatching = np.zeros((n,n), dtype=int)\nfor i in range(n):\n    for j in range(n):\n        matching[i,j] = pulp.value(x[i,j])\n\n# Print solution and objective value\nprint(\"The optimal matching is: \\n\", matching)\nprint(f\"Optimal value of the objective function: {pulp.value(prob.objective)}\")\n\n\n\n\n5.2.3 Remarks\nThe linprog package is typically convenient for small linear optimization problems, especially for problems whose input data (such as the constraint matrices) you can define explicitly.\nThe pulp package is more useful when the input data is defined implicitly, e.g., using different sets of inequalities defined throug indices. Furthermore, when problems become of larger scale, as you will typically encounter in practice, it is better to use state-of-the-art solvers such as CPLEX, GUROBI, or MOSEK. PuLP is able to be coupled to such solvers, that is, you can define a problem in PuLP and then have it solved with the external solver software.\nMany of these solvers can also handle other problems such as mixed integer linear optimization problems, in which there is a mixture of continuous, integer and binary variables, and various non-linear optimization problems. We will see more of those in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "06-nonlinear.html",
    "href": "06-nonlinear.html",
    "title": "6  Nonlinear algebra and optimization",
    "section": "",
    "text": "6.1 Root finding\nIn this section we will discuss various root finding methods. We start by introducing various methods for finding the root of a univariate funtion, and then show similar methods for the multivariate case. We end with the least squares method, that finds an approximately optimal solution when a solution might not exist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nonlinear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "06-nonlinear.html#root-finding",
    "href": "06-nonlinear.html#root-finding",
    "title": "6  Nonlinear algebra and optimization",
    "section": "",
    "text": "6.1.1 Univariate function\nWe present three type of methods and information that can be used to find roots of a function. We emphasize that none of these methods guarantee that a root will always be found if it exists, but in many cases they do.\n\nUsing fsolve()\nThe easiest-to-use function for finding the root of a univariate function f : \\mathbb{R} \\rightarrow \\mathbb{R} is fsolve() from the optimize module. It takes two mandatory input arguments:\n\nThe function we want to find the root of, and\nan initial guess for the root.\n\nAs an example, suppose we want to find the root of the equation \nf(x) = x^2 + 2\\cdot x - 1,\n that is plotted below. You do not have to look at the code generating this figure (but it is included for completeness).\n\n\n\nShow code generating the plot below\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the x range\nx = np.linspace(-3, 3, 600)\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n\n# Create the plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, f(x), label='$f(x) = x^2 + 2x - 1$')\n\n# Add labels and title\nplt.title('Plot of the function f on the interval [-3,3]')\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n# Add a grid\nplt.grid(True)\n\n# Set range\nplt.xlim(-3,3)\nplt.ylim(-4,14)\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOur initial guess will be guess = 3. There are various ways in which you can input the function and initial guess. Either as\n\nKeyword arguments: Use func for the function and x0 for the initial guess.\nPositional arguments: Input function followed by initial guess, without keywords.\n\n\ndef f(x):\n    return x**2 + 2*x - 1\n\nguess = 3\nx_root = optimize.fsolve(f,x0=guess) \n\n# Also works:\n# x_root = optimize.fsolve(func=f,x0=guess) \n# x_root = optimize.fsolve(x0=guess, func=f) \n# x_root = optimize.fsolve(f,guess), here the order matters!\n\n# Does not work\n# x_root = optimize.fsolve(x0=guess,f), f is positional, order matters!\n# x_root = optimize.fsolve(guess,f), order matters!\n \nprint(\"A root of the function f is given by\", x_root)\n\n# x_root is an array one one element; \n# can access index 0 to only get value\nprint(\"A root of the function f is given by\", x_root[0])\n\nA root of the function f is given by [0.41421356]\nA root of the function f is given by 0.41421356237309503\n\n\nFor any function, in SciPy and beyond, you should look at the order of the required arguments of a function in its documentation.\n\nBecause different root finding and optimization methods use a different keyword argument for the function of interest (e.g., func, fun, f), we will introduce the convention in this chapter that the first input argument of any SciPy function that we use is the function we want to find the root of, or optimize over, and all other input arguments will have a keyword (such as x0). The order of the keyword input arguments is not relevant, as long as they come after the function, which is the first input argument.\n\nThe choice of initial guess can determine which root we end up in. The function f has two roots as can be seen from the figure. If we start with another initial guess, such as guess=4, we find the other root of f.\n\ndef f(x):\n    return x**2 + 2*x - 1\n\nguess = -4\nx_root = optimize.fsolve(f,x0=guess) \n\nprint(\"Another root of the function f is given by\", x_root)\n\nAnother root of the function f is given by [-2.41421356]\n\n\nIf the function f would have had multiple input arguments, than fsolve() interprets the first argument of f as being the unknown variable that it needs to compute. Any remaining arguments of f should be specified in the args keyword argument.\nFor example, suppose that we would have defined g(x,a,b,c) = a\\cdot x^2 + b\\cdot x + c. Then executing x_root = optimize.fsolve(g,x0=guess) will result in an error because fsolve() cannot determine a root x if it does not know the values of a,b and c. Therefore, we need to specify these additional inputs in the args keyword argument of fsolve().\n\ndef g(x,a,b,c):\n    return a*x**2 + b*x + c\n\na, b, c = 1, 2, -1\n\nguess = -4\nx_root = optimize.fsolve(g,x0=guess,args=(a,b,c))\n\nprint(\"A root of the function g is given by\", x_root)\n\nA root of the function g is given by [-2.41421356]\n\n\nWe could have also stored the parameters a,b,c in an array so that g would have only had one additional input argument. This is illustrated below.\n\ndef g(x,coeff):\n    return coeff[0]*x**2 + coeff[1]*x + coeff[2]\n\ncoeff = np.array([1,2,-1])\n\nguess = -4\nx_root = optimize.fsolve(g, x0=guess, args=(coeff))\n\nprint(\"A root of the function g is given by\", x_root)\n\nA root of the function g is given by [-2.41421356]\n\n\n\n\n\nBracket information\nAnother function that can find the root of a univariate function is root_scalar() from the optimize module.\nWhereas fsolve() uses one fixed method in the background to find a root, root_scalar() allows the user to choose from a collection of methods. Some methods might perform better than others, depending on the type of function you are trying to find a root of. You can specify which method you want to use with the method keyword argument. Some methods require additional keyword arguments to be specified; see the documentation.\nOne such additional keyword argument is bracket, that allows you to specify the interval, or bracket, [a,b] in which the root should be searched for. This does, however, come with the requirement that the function values in the points a and b should have a different sign: Either f(a) &lt; 0 &lt; f(b) or f(b) &lt; 0 &lt; f(a). The reason is that this guarantees, by the Intermediate Value Theorem, that there is at least one root in the interval [a,b]. If the bracket does not satisfy this condition, then Python will raise an error (try this yourself).\nLet us look at an example where we use the Bisection method, called 'bisect' in SciPy, with interval [0,4]. The order in which you place the keyword argument bracket and method does not matter.\n\ndef f(x):\n    return x**2 + 2*x - 1\n\ninterval = [0,4]\n\nresult = optimize.root_scalar(f, bracket=interval, method='bisect') \nprint(result)\n\n      converged: True\n           flag: 'converged'\n function_calls: 43\n     iterations: 41\n           root: 0.41421356237151485\n\n\nNote that Python returns a lot of information regarding the root finding process. For example, it tells us whether the process has converged, meaning it found a point that satisfies f(x) = 0 up to a default precision. You can access these properties with the syntax result.property_name where property_name is the property of interest.\n\nprint(\"The root finding process converged:\", result.converged)\n\nThe root finding process converged: True\n\n\nFor us, the most important property is root, which gives the value of the root.\n\nprint(\"A root of the function f is given by\", result.root)\n\nA root of the function f is given by 0.41421356237151485\n\n\n\n\nDerivative information\nIf a function is differentiable, it is also possible to specify its derivative in some methods. This typically results in much faster root finding methods.\nRecall, for example, Newton’s method from Section Section 2.4 that finds a root by iteratively computing better approximations using the formula \nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}\n starting from some initial guess x_0. The formula to compute the next iterate relies on\n\nThe derivative f'(x), and\nan initial guess x_0.\n\nLet us look at an example of using Newton’s method. The derivative f' should be defined as a function and can be entered in the fprime keyword argument. The initial guess is inputted in the x0 keyword argument.\nFor our function f(x) = x^2 + 2x - 1 we have f'(x) = 2(x+1).\n\ndef f(x):\n    return x**2 + 2*x - 1\n    \ndef f_deriv(x):\n    return 2*(x+1)\n\nguess = 4\nresult = optimize.root_scalar(f,  method='newton', \\\n                                fprime=f_deriv, x0 = guess) \nprint(result)\n\n      converged: True\n           flag: 'converged'\n function_calls: 14\n     iterations: 7\n           root: 0.41421356237309503\n\n\nAlthough it is a little bit like comparing apples and pears, the number of function calls and iterations (determining how long a method needs to converge) of Newton’s method is much lower than that of the Bisection method.\nFinally, we remark that the args keyword argument to specify additional input parameters can also be used in combination with methods that use bracket or derivative information.\n\n\n\n6.1.2 Multivariate functions\n\nUsing fsolve()\nThe fsolve() function can also be used to compute a root of a system of n functions with n unknown variables. It again takes two input arguments:\n\nThe system of function equations to be solved, and\nan initial guess for the (unknown) root.\n\nThe system of equations should be modelled as a Python function, i.e., we need a function that takes as input an array x = [x_0,\\dots,x_{n-1}] and outputs the array f(x) = [f_{0}(x),\\dots,f_{n-1}(x)]. This function will then be the input for fsolve().\nAs an example for n = 2, suppose we want to solve the system\n\n\\left\\{\\begin{array}{rll}\nx_0^2 + x_1^2 &=& 4 \\\\\nx_0 + x_1 &=& 1 \\\\\n\\end{array}\\right..\n\nThat is, we have f_0(x_0,x_1) = x_0^2 + x_1^2 -4 and f_1(x_0,x_1) = x_0 + x_1  - 1, and want to solve f_0(x) = 0, f_1(x) = 0. The array [f_0(x),f_1(x)] can be defined as a function in the following way.\n\ndef f(x):\n    # Input  : Array x = [x_0, x_1]\n    # Output : Array f = [f_0(x), f_1(x)]\n    \n    f = np.array([x[0]**2 + x[1]**2 - 4, x[0] + x[1] - 1]) \n    return f\n\nWe emphasize that here the array x = [x_0,x_1] is the input of the function, and not x_0 and x_1 separately. If we would define f as a function of two inputs, i.e., f(x_0,x_1), then fsolve() would want to find a root with respect to its first argument x_0 only, which is not what we want.\nUsing fsolve() to do the root finding gives us the following solution.\n\nguess = np.array([1,1]) # Our initial guess\nroot = optimize.fsolve(f, x0=guess)\n\nprint(root)\n\n[ 1.82287566 -0.82287566]\n\n\nNote that the initial guess is an array in \\mathbb{R}^2 this time, as we are considering a function with two variables.\nAlso here we can use the args keyword argument to specify additional input parameters. Suppose we want to solve, for a = 2 and b = 4, the system\n\n\\left\\{\\begin{array}{rll}\na\\cdot x_0^2 + x_1^2 &=& 4 \\\\\nx_0 + b\\cdot x_1 &=& 1 \\\\\n\\end{array}\\right..\n\n\ndef f(x,a,b):\n    return np.array([a*x[0]**2 + x[1]**2 - 4, x[0] + b*x[1] - 1]) \n    \nguess = np.array([1,1]) # Our initial guess\na, b = 2, 4\nx_root = optimize.fsolve(f, x0=guess, args=(a,b))\n\nprint(x_root)\n\n[ 1.41233385 -0.10308346]\n\n\nYou can double-check that the root x^* you found is indeed a root by plugging the solution into the system of equations, i.e., checking if it satisfies \nf(x^*) = [f_0(x^*), \\dots,  f_{n-1}(x^*)] = [0,\\dots,0].\n\n\nprint(f(x_root,a,b)) # Both coordinates approximately equal to zero\n\n[9.59232693e-14 0.00000000e+00]\n\n\n\n\nDerivative information\nJust as in the univariate case, it is also possible to use other function for finding a root. The analogue of root_scalar() is the function root(). Although it is not possible to input bracket information for this function, it does have methods that use derivate information. These methods are typically faster than fsolve().\nConsider again the system\n\n\\left\\{\\begin{array}{rll}\nx_0^2 + x_1^2 - 4 &=& 0 \\\\\nx_0 + x_1 -1 &=& 0 \\\\\n\\end{array}\\right..\n\nThe “derivative” of the function f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n given by f(x) = [f_0(x),\\dots,f_{n-1}(x)] is the Jacobian matrix\n\nJ(f) =\n\\frac{\\partial (f_0, \\cdots,f_{n-1})}{\\partial (x_0,\\cdots,x_{n-1})} =\n\\begin{bmatrix}\n\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\cdots & \\frac{\\partial f_0}{\\partial x_n} \\\\[1ex]\n\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots                          \\\\\n\\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} & \\cdots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n\\end{bmatrix}.\n\nFor the two equations at hand, we have\n\nJ(f) = \\begin{bmatrix} 2x_0 & 2x_1\\\\ 1 & 1\\end{bmatrix}.\n\nJust as in the univariate case when using Newton’s method, we need to input an initial guess and the Jacobian matrix, where the latter should be defined as a function. The Jacobian is inputted in the jac keyword argument; the initial guess in x0.\n\ndef f(x):    \n    return np.array([x[0]**2 + x[1]**2 - 4, x[0] + x[1] - 1]) \n\ndef jac_f(x):\n    J = np.array([[2*x[0], 2*x[1]],[1,1]])\n    return J\n    \nguess = np.array([1,1]) # Our initial guess\nresult = optimize.root(f, jac=jac_f,x0=guess)\n\nprint(result) \n\n message: The solution converged.\n success: True\n  status: 1\n     fun: [ 0.000e+00  0.000e+00]\n       x: [ 1.823e+00 -8.229e-01]\n    nfev: 27\n    njev: 2\n    fjac: [[-9.745e-01 -2.243e-01]\n           [-2.243e-01  9.745e-01]]\n       r: [-4.458e+00  6.983e-01  1.187e+00]\n     qtf: [ 1.848e-10  4.255e-11]\n\n\nWe can access the properties of the result of the root finding procedure with the syntax result.property_name where property_name is the property of interest. Here the root is given by the property x.\n\nprint(\"A root of the function f is given by\", result.x)\n\nA root of the function f is given by [ 1.82287566 -0.82287566]\n\n\nNote that this is the same root that we found with fsolve().\n\n\n\n6.1.3 Least squares method\nIf the number of functions is not equal to the number of variables, or if the system does not have a solution, the least squares method optimize.least_squares is your best pick to find a root.\nThe function (when using the default settings) tries to compute a point x that attains the minimum of the residual function \nR(f_0,\\dots,f_{n-1}) = \\min_{x \\in \\mathbb{R}^n}  \\sum_{i=1}^{n-1} f_i(x)^2,\n It should be observed that R(f_0,\\dots,f_{n-1})  \\geq 0 and R(f_0,\\dots,f_{n-1}) = 0 if and only if the system f_0(x) = 0,\\dots, f_{n-1}(x)=0 has a root.\nThe function optimize.least_squares takes just like fsolve() two input arguments: The system of equations and an initial guess.\nLet’s look at an example. Consider the system \n\\left\\{\n\\begin{array}{lllll}\nf_0(x)& =& \\sin(x) &=& 0 \\\\\nf_1(x)& =& (x - \\pi + 0.1)^2 &=& 0 \\\\\nf_2(x)& =& (x - \\pi) &=& 0\n\\end{array}\\right..\n Without the 0.1-term, this system would have the unique solution x = \\pi, but this small perturbation causes it to become infeasible. We can still find an approximately optimal solution with the least squares method. The number \\pi can be accessed in NumPy using np.pi.\n\ndef system(x):\n    return np.array([np.sin(x[0]), (x[0]-np.pi + 0.1)**2, x[0] - np.pi])\n    \nguess = 1    \nresult = optimize.least_squares(system,x0=guess)\n\nprint(\"Least squares solution found is x = \", result.x) # Close to pi\n\nLeast squares solution found is x =  [3.1406215]\n\n\nAlthough optimize.least_squares often works well, it does not necessarily compute a root of the function even if one exists. What it actually does, is compute a local minimum of the residual function.\nLet us illustrate this with an example. Consider the function f(x) = x^4 + 0.3x^3 - 2.5x^2 + 1.5, which has two (real) roots as can be seen from the figure below.\n\n\n\nShow code generating the plot below\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the x range\nx = np.linspace(-2, 2, 600)\n\n# Define the function f\ndef f(x):\n    return x**4 + 0.3*x**3 - 2.5*x**2 + 1.5\n\n# Create the plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, f(x), label='$f(x) = x^4 + 0.3x^3 - 2.5x^2 + 1.5$')\n\n# Add labels and title\nplt.title('Plot of the function f on the interval [-2,2]')\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n# Add a grid\nplt.grid(True)\n\n# Set range\nplt.xlim(-2,2)\nplt.ylim(-1,3)\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe execute both Newton’s and the least squares method with the same initial guess.\n\n# Function f\ndef f(x):\n    return (x**4 + 0.3*x**3 - 2.5*x**2 + 1.5)\n\n# Derivative of f    \ndef f_deriv(x):\n    return 4*x**3 + 0.9*x**2 -5*x\n\nguess = 1.5\n\n# Newton's method\nresult = optimize.root_scalar(f,  method='newton', \\\n                                fprime=f_deriv, x0 = guess) \nprint(\"Root found by Newton's method: \\n x =\", result.root,  \n        f\"with f(x) =\", f(result.root))\n\n# Least squares method\nresult_ls = optimize.least_squares(f, x0=guess) \nprint(\"Root found by least squares method: \\n x =\", result_ls.x,  \n        f\"with f(x) =\", f(result_ls.x))\n\nRoot found by Newton's method: \n x = -1.5180670079327394 with f(x) = -8.881784197001252e-16\nRoot found by least squares method: \n x = [1.01118148] with f(x) = [0.29943799]\n\n\nAs you can see, the least squares method does not find a root, because the function value in the computed point is almost 0.3. What goes wrong here? Consider the residual problem \nR(f) = \\min_x (x^4 + 0.3x^3 - 2.5x^2 + 1.5)^2.\n\nThe function g(x) = f(x)^2 = (x^4 + 0.3x^3 - 2.5x^2 + 1.5)^2 is plotted below. As you can see, it has a local minimum around 1. Because we started out with the initial guess 1.5, the least squares method gets stuck in this local minimum.\n\n\n\nShow code generating the plot below\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the x range\nx = np.linspace(-2, 2, 600)\n\n# Define the function f\ndef f(x):\n    return (x**4 + 0.3*x**3 - 2.5*x**2 + 1.5)**2\n\n# Create the plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, f(x), label='$f(x)^2 = (x^4 + 0.3x^3 - 2.5x^2 + 1.5)^2$')\n\n# Add labels and title\nplt.title('Plot of the function f on the interval [-2,2]')\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n# Add a grid\nplt.grid(True)\n\n# Set range\nplt.xlim(-2,2)\nplt.ylim(-1,3)\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n To close this section, we again emphasize that no method is guaranteed to always find a root for every function. For example, fsolve() is also not able to find a root of the function f in this section (try this yourself).\n\nTherefore, always check whether a found solution is actually a root by evaluating the function in the solution found and checking if the resulting value is (almost) zero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nonlinear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "06-nonlinear.html#nonlinear-optimization",
    "href": "06-nonlinear.html#nonlinear-optimization",
    "title": "6  Nonlinear algebra and optimization",
    "section": "6.2 Nonlinear optimization",
    "text": "6.2 Nonlinear optimization\nIn this section we consider the minimization problem \n\\min_{x \\in \\mathbb{R}^n} f(x).\n for a function f : \\mathbb{R}^n \\rightarrow \\mathbb{R}. Note that maximization can be done by considering the function -f.\nWe again start with univariate functions and then switch to multivariate functions later on. In the last section we will also see some examples of constrained optimization, where the domain of f is a subset of \\mathbb{R}^n. The syntax for using minimization functions from SciPy will be similar to the syntax we saw for root finding methods.\nIn particular, also here we can use the args keyword argument to specify any additional input arguments of the function we want to minimize over, so that the optimization can happen with respect to the unknown first input argument of the function.\n\n6.2.1 Univariate function\nIf f : \\mathbb{R} \\rightarrow \\mathbb{R} is a univariate function then the easiest-to-use functions for minimization are fmin() and minimize_scalar() from the optimize module. We emphasize that these functions find a local minimum of the function f, which might or might not be a global minimum.\nThe function fmin() works similar as fsolve() in that it requires an initial guess.\n\ndef f(x):\n    return x**4 + 0.3*x**3 - 2.5*x**2 + 1.5\n\nguess=1.5\nx_min = optimize.fmin(f,x0=guess)\n\nprint(\"The minimum found is x =\", x_min) # Local minimum\n\nOptimization terminated successfully.\n         Current function value: 0.299438\n         Iterations: 15\n         Function evaluations: 30\nThe minimum found is x = [1.01118164]\n\n\nWith a different initial guess the method is able to find the global minimum; see the figure in the previous section. Furthermore, you can suppress the output message of fmin() by setting the keyword argument disp=False; see the documentation of fmin().\n\nguess=-1.5\nx_min = optimize.fmin(f,x0=guess,disp=False)\n\nprint(\"The minimum found is x =\", x_min) # Local minimum\n\nThe minimum found is x = [-1.23618164]\n\n\nThe function minimize_scalar() gives more flexibility in terms of input arguments. First of all, we can execute it without any additional arguments, but then it more easily gets stuck in a local minimum.\n\nresult = optimize.minimize_scalar(f)\nprint(result)\n\n# We access root with result.x \nprint(\"The minimum found  is x =\", result.x) \n\n message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 0.29943799106751934\n       x: 1.011179780213775\n     nit: 11\n    nfev: 14\nThe minimum found  is x = 1.011179780213775\n\n\nWe can also set an interval in which we want to find a minimum using the bounds keyword argument. Note that in the example below, the function minimize_scalar() does not get stuck in the local minimum around 1.\n\ninterval = [-2,2]\nresult = optimize.minimize_scalar(f,bounds=interval)\n\nprint(\"The minimum found  is x =\", result.x) \n\nThe minimum found  is x = -1.2361799028533706\n\n\nFor more options and different methods that can be used with the method keyword argument, see the documentation of minimize_scalar.\n\n\n6.2.2 Multivariate function\nFor multivariate functions, we can use the minimize() function from optimize, being the analogue of root() for root finding. This function can take into account derivative, or gradient, information with certain methods. The gradient of f is given by \n\\nabla f(x) = \\left[\\frac{\\partial f}{\\partial x_0}, \\dots, \\frac{\\partial f}{\\partial x_{n-1}} \\right].\n We can input the gradient again with the keyword argument jac. The default method that minimize() uses is 'BFGS' and this method can use gradient information. You can choose your own method with the method keyword argument; see the documentation.\n\n# Define the function\ndef f(x):\n    return (x[0] - 3)**2 + (x[1] + 4)**2 - 1\n\n# Define the gradient\ndef grad_f(x):\n    return np.array([2 * (x[0] - 3), 2 * (x[1] + 4)])\n\n# Initial guess\nguess = np.array([0.0, 0.0])\n\n# Minimization with gradient information\nresult = optimize.minimize(f, x0=guess, jac=grad_f)\n\n# Print the result\nprint(\"The minimum found is:\", result.x)\nprint(\"Function value at minimum:\", result.fun)\n\nThe minimum found is: [ 3. -4.]\nFunction value at minimum: -1.0\n\n\n\n\n6.2.3 Constrained optimization\nJust as with minimize_scalar() the function minimize() can also include interval information using the bounds keyword argument. The syntax for this argument is a list of tuples that for every variable in x = [x_0,\\dots,x_{n-1}] contains the lower and upper bound value for the variable. Recall that this is the same way how variable bounds were specified in the linprog package for solving (integer) linear optimization problems.\nThat is, if we have the constraints \\ell_i \\leq x_i \\leq u_i for i = 0,\\dots,n-1, then the input for bounds is [(l_0, u_0), (l_1, u_1), ..., (l_{n-1},u_{n-1})]. Just as in linprog you can set an upper or lower bound equal to None to model that the lower bound is -\\infty, or that the upper bound is +\\infty.\n\n# Define the function\ndef f(x):\n    return (x[0] - 3)**2 + (x[1] + 4)**2 - 1\n\n# Define the gradient\ndef grad_f(x):\n    return np.array([2 * (x[0] - 3), 2 * (x[1] + 4)])\n\n# Initial guess\nguess = np.array([0.0, 0.0])\nintervals = [(None,2), (-2,None)] # x_0 &lt;= 2, x_1 &gt;= -2\n\n# Minimization with gradient information\nresult = optimize.minimize(f, x0=guess, bounds=intervals)\n\n# Print the result\nprint(\"The minimum found is:\", result.x)\nprint(\"Function value at minimum:\", result.fun)\n\nThe minimum found is: [ 2. -2.]\nFunction value at minimum: 4.0\n\n\nNext to specifying variable bounds, minimize() can also take into account more complex constraints that restrict the domain of f, i.e., the search space of minimize(), using the constraints keyword argument.\nThat is, we can solve the problem\n\n\\begin{array}{lll}\n\\min_{x \\in \\mathbb{R}^n} & f(x)  &\\\\\n\\text{s.t.} & g_j(x) \\geq 0 &  \\text{ for } j = 1,\\dots,q-1\\\\\n& h_k(x) = 0 & \\text{ for } k = 0,\\dots,r-1\n\\end{array}.\n\nThe syntax for adding constraints is to use a tuple containing dictionaries, where each dictionary models a constraints with keys\n\n'type': Use 'ineq' for a \\geq-constraint and 'eq' for an =- constraint;\n'fun' : Python function that describes g_j or h_k.\n\nThere is also the optional keyword argument 'args' that allows you to specify additional parameters that appear in the function g_j or h_k.\nAs an example, suppose we want to solve the problem\n\n\\begin{array}{lll}\n\\min_{x \\in \\mathbb{R}^n} & x_0^2 + 5x_1^2  &\\\\\n\\text{s.t.} & x_0 + x_1 \\geq 5 &  \\\\\n& x_0 - 2x_1 = 3 &\n\\end{array}\n\nThen we have g_0(x) = x_0 + x_1 - 5 and h_0(x) = x_0 - 2x_1 - 3.\n\n# Define functions of constraints\ndef g(x):\n    return x[0] + x[1] - 5\n\ndef h(x):\n    return x[0] - 2*x[1] - 3\n\n# Define tuple containing constraints as dictionaries\ncons = ({'type' : 'ineq', 'fun' : g}, \n        {'type' : 'eq', 'fun' : h})\n\nLet us now do the optimization using these constraints.\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n\nguess = np.array([0,0]) # minimize() always needs guess\n\nresult = optimize.minimize(f, x0=guess, constraints=cons)\n\nprint(\"The minimum found is:\", result.x)\nprint(\"Function value at minimum:\", result.fun)\n\nThe minimum found is: [4.33333333 0.66666667]\nFunction value at minimum: 19.2222222222229\n\n\nThis allows us, for example, to optimize a nonlinear function subject to linear constraints.\n\n\n6.2.4 Remarks\nJust as in the previous chapter, we remark here that there are various other packages in Python to perform optimization tasks with. For example, there is cvxpy for convex optimization, and you can also couple external solvers with Python, as explained in the last chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nonlinear algebra and optimization</span>"
    ]
  },
  {
    "objectID": "07-visualization.html",
    "href": "07-visualization.html",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1 Basic plotting\nIn this section we will explain step-by-step how to generate a nice looking figure containing a visualization of a one-dimensional function. We start with plotting the function f(x) = x^2 + 2x -1 for some values of x in a two-dimensional figure.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n    \n# Define the x range of x-values\nx = np.array([-3,-2,-1,0,1,2,3])\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.plot(x, y)\n\n# Show the plot\nplt.show()\nYou can view the figure in the Plots pane (or tab) in Spyder.\nWe will next explain what the code above is doing. After defining the function f, we create the vector (i.e., Numpy array) \nx = [x_1,x_2,x_3,x_4,x_5,x_6,x_7] =  [-3,-2,-1,0,1,2,3].\nBecause the function f is vectorized, we can right away compute all the function values in these points. We store them in the array y = f(x), that is,\n\\begin{array}{ll}\ny = f(x) & = [f(x_1),f(x_2),f(x_3),f(x_4),f(x_5),f(x_6),f(x_7)]  \\\\\n& = [2,-1,-2,-1,2,7,14].\n\\end{array}\nNext, we create an (empty) figure using the command plt.figure(). Then comes the most important command, plt.plot(x,y), that plots the elements in the vector x against the elements in the vector y = f(x), and connects consecutive combinations (x_i,y_i) and (x_{i+1},y_{i+1}) with a line segment. For example, we have (x_1,y_1) = (-3,2) and (x_2,y_2) = (-2,-1). The left most line segment is formed by connecting these points.\nIf you only want to plot the points (x_i,y_i), and not the line segments, you can use plt.scatter(x,y) instead of plt.plot(x,y).\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n    \n# Define the x range of x-values\nx = np.array([-3,-2,-1,0,1,2,3])\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.scatter(x, y)\n\n# Show the plot\nplt.show()\nObserve that the (blue) line in the figure that was generated using plt.plot(x,y) is not very “smooth”, i.e., the function visibly is connected by line segments. To get a smoother function line, we can include more points in the vector x. This can be done, for example, with the linspace() function that we have seen in Chapter 3.\nLet us plot again the function f, but this time with 600 elements in x in the interval [-3,3]. We use plt.plot() again, instead of plt.scatter(). We now obtain a much smoother function line.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n    \n# Define the x range of x-values\nx = np.linspace(-3,3,600)\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.plot(x, y)\n\n# Show the plot\nplt.show()\nYou can add a legend for the line/points that you plot by using the label argument of plt.plot(). For example we can add the function description using plt.plot(x,y,label='$f(x) = x^2 + 2x - 1$'). This is in particular useful if you plot multiple functions in one figure, as the example below illustrates. There we plot the functions f and g, with g(x) = 3x a new function. To have the labels appear in the legend of the figure, you need to add a legend to the figure with plt.legend().\nIf you want to add labels to the horizontal and vertical axis, you can use the commands plt.xlabel() and plt.ylabel().\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n\n# Define the function g\ndef g(x):\n    return 3*x\n    \n# Define the x range of x-values\nx = np.linspace(-3,3,600)\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\nz = g(x)\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.plot(x, y, label='$f(x) = x^2 + 2x - 1$')\nplt.plot(x, z, label='$g(x) = 3x$')\n\n# Create labels for axes\nplt.xlabel('x')\nplt.ylabel('Function value')\n\n# Create the legend with the specified labels\nplt.legend()\n\n# Show the plot\nplt.show()\nYou might observe that the range on the vertical axis changed now that we added a second function to the plot. When we only plotted the function f, the vertical axis ranged from -2 to 14, but now with the function g added to it, it ranges from -10 to 15.\nYou can fix the range [c,d] on the vertical axis using the command plt.ylim(c,d), and to fix the range of the horizontal axis to [a,b], you can use plt.xlim(a,b). In the figure below, we fix the vertical range to [c,d] = [-10,14] and the horizontal axis to [a,b] = [-3,3].\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n\n# Define the function g\ndef g(x):\n    return 3*x\n    \n# Define the x range of x-values\nx = np.linspace(-3,3,600)\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\nz = g(x)\n\n#Create the figure object\nplt.figure()\n\n# Create the plot within the figure\nplt.plot(x, y, label='$f(x) = x^2 + 2x - 1$')\nplt.plot(x, z, label='$g(x) = 3x$')\n\n# Create labels for axes\nplt.xlabel('x')\nplt.ylabel('Function value')\n\n# Create the legend with the specified labels\nplt.legend()\n\n# Fix the range of the axes\nplt.xlim(-3,3)\nplt.ylim(-10,14)\n\n# Show the plot\nplt.show()\nFinally, you can also add a title to the plot using the command plt.title() as well as a grid in the background of the figure using plt.grid(). These are illustrated in the figure below.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f\ndef f(x):\n    return x**2 + 2*x -1\n\n# Define the function g\ndef g(x):\n    return 3*x\n    \n# Define the x range of x-values\nx = np.linspace(-3,3,600)\n\n# Compute the function values f(x[i]) of the elements x[i] \n# and store them in the array y\ny = f(x)\nz = g(x)\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.plot(x, y, label='$f(x) = x^2 + 2x - 1$')\nplt.plot(x, z, label='$g(x) = 3x$')\n\n# Create labels for axes\nplt.xlabel('x')\nplt.ylabel('Function value')\n\n# Create the legend with the specified labels\nplt.legend()\n\n# Fix the range of the axes\nplt.xlim(-3,3)\nplt.ylim(-10,14)\n\n# Add title to the plot\nplt.title('A first plot in Python of two functions')\n\n# Add grid to the background\nplt.grid()\n\n# Show the plot\nplt.show()\nThis completes the description of the basics of plotting a figure. As a final remark, there are many more plotting options that we do not cover here, but which can be found in the documentation. For example, with the plt.xticks() and plt.yticks() commands you can specify the numbers you want to have displayed on the horizontal and vertical axis, respectively. Also, there are commands to specify line color, width, type (e.g., dashed) and much more!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "07-visualization.html#basic-plotting",
    "href": "07-visualization.html#basic-plotting",
    "title": "7  Visualization",
    "section": "",
    "text": "If the resolution of the plots in the Plots pane does not seem good enough, you can increase it by going to “Tools &gt; Preferences &gt; IPython console &gt; Graphics &gt; Inline backend &gt; Resolution” and set the resolution to, for example, 300 dpi.\n\n\nYou can get the Plots pane in fullscreen by going to the button with the three horizontal lines in the top right corner and choose “Undock”. You can “Dock” the pane again as well if you want to leave the fullscreen mode.\n\n\n\n\nIPython Console",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "07-visualization.html#subplots",
    "href": "07-visualization.html#subplots",
    "title": "7  Visualization",
    "section": "7.2 Subplots",
    "text": "7.2 Subplots\nIn this section we will describe how you can create multiple subplots in one figure. There are various ways to do this, e.g., in a predefined grid or on a plot-by-plot basis.\n\n7.2.1 Fixed grid\nWe start with explaining the basics of the subplots() function. The syntax for creating a figure with a predefined grid on which plots can be placed is as follows.\n\n\nm, n = 2, 3\n\n# Create figure with six subplots in an n x m grid\nfig, ax = plt.subplots(m,n)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis creates a figure object, with name fig in this case, and a 2 \\times 3 array ax with so-called Axes objects that is place inside the figure. We are going to place the plots on the positions of the ax array.\n\n# Shape of array\nprint(np.shape(ax))\n\n(2, 3)\n\n\nThe fact that arrays can also store other objects besided numbers, is something we also already saw when using the pulp package for linear optimization in Chapter 5.\nOne might argue that the figure above is visually not very appealing, especially because the horizontally adjacent plots are very close to each other. You can get more control over the size of the figure (in which the plots are placed) by using the figsize keyword whose argument should be a tuple (w,h) indicating the width w and the height h of the figure.\nNote that the input arguments of figsize are perhaps a bit counterintuitive, as for the shape of a NumPy array (like the command above) the first number is always the “height” of the matrix, and the second number the “width”, but this is the other way around for the measurements of a figure.\n\n\n# Parameters for figure with n x m subplots\nm, n = 2, 3\n\n# Parameters w (width) and h (height) for figure size \nw, h = 12, 4\n\n# Create figure with six subplots in a 2 x 3 fashion\nfig, ax = plt.subplots(m,n, figsize=(w,h))\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nIt usually also helps to put in the command plt.tight_layout() that prevents plots within a figure from overlapping by addings some spacing between them.\n\n\n# Parameters for figure with n x m subplots\nm, n = 2, 3\n\n# Parameters w (width) and h (height) for figure size \nw, h = 12, 4\n\n# Create figure with six subplots in a 2 x 3 fashion\nfig, ax = plt.subplots(m,n, figsize=(w,h))\n\n# Tighten layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n We continue by explaining how you can add information to the indiviual plots in the figure. For this, we will switch to a figure with a 2 \\times 2 array for in total four subplots.\n\n\n# Parameters for figure with n x m subplots\nn = 2\n\n# Parameters w (width) and h (height) for figure size \nw = 5\n\n# Create figure with six subplots in a 2 x 2 fashion\nfig, ax = plt.subplots(n,n, figsize=(w,w))\n\n# Tighten layout\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nYou can access the individual plot at position (i,j) of the array ax using ax[i,j], and set properties of it using ax[i,j].plot_option where plot_option is a plotting command.\nSometimes you need to use a slightly different command than when you plot a single plot in a figure. For many commands, you need to add set_ to it. For example, instead of plt.xlim(a,b) you need to use ax[i,j].set_xlim(a,b).\nTo set the title of the whole figure, when named fig, you can use fig.suptitle() instead of plt.title().\n\n\n# Define function to plot\ndef f(x):\n    return x**2\n\na = -3\nb = 3\n\n# Define x-range\nx = np.linspace(a,b,600)\n\n# Create figure with six subplots in a 2 x 2 fashion\nfig, ax = plt.subplots(n,n, figsize=(w,w))\n\n# Title of whole figure\nfig.suptitle(\"Four plots in one figure\")\n\n# Tighten layout\nplt.tight_layout()\n\n# Create plot on top-left position\nax[0,0].plot(x,f(x))\nax[0,0].set_xlim(a,b)\nax[0,0].set_ylim(0,9)\nax[0,0].set_xlabel(\"x\")\nax[0,0].set_ylabel(\"f(x)\")\nax[0,0].set_title(\"Plot of function $f(x) = x^2$\")\nax[0,0].grid()\n\n# Tighten layout\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.2.2 Iterative adding\nInstead of predefining a grid in which the subplots will appear, it is also possible to add subplots in a more dynamic, iterative fashion to a grid using add_subplot(). The function typically gives you a bit more flexibility.\nFor example, we can create four plots of which one spans the whole first “row” and that has three smaller plots underneath it. The numbering of the subplots follows the largest axis changing fasted principle, so the subplots are placed first along the first row, then the second row, etc.\nIf we would have a 2 \\times 3 grid, then the numbering of the subplots would be as follows:\n\n\\left[\n\\begin{array}{ccc}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{array}\n\\right]\n\nNote that the counting start at 1 instead of 0, as is more common in Python.\nAfter having created a figure, we can add subplots to an m \\times n grid using fig.add_subplot(m,n,(p,q)). The tuple (p,q) indicates that we want to place the subplot on positions p through q in the figure. If there is only one position p at which you want to place the subplot, you can use (p), or simply p, as the third argument of add_subplot().\nTo avoid unnecessary repetition, it can often help to plot subplots using a for-loop. We will illustrate this for the figure below, in which we plot the function f(x) = \\sin(x) on the first row or our grid, and its first three derivatives in smaller subplots under it.\nYou can make the plot look nicer by adding labels, legends, different line colors, a grid, etc.\n\n\n# Define x-range\nx = np.linspace(-5,5,600)\n\n# Function values\ny = np.sin(x)\n\n# Values of 1st, 2nd and 3rd derivative\nderiv = np.vstack((np.cos(x),-np.sin(x),-np.cos(x)))\n\n# Store function names in list\nfunction_names = [\"Function f\", \"First derivative\", \n                  \"Second derivative\", \n                  \"Third derivative\"]\n\n# Create figure\nfig = plt.figure(figsize=(7,5))    \n\n# Will create an m x n grid with subplots\nm, n = 2, 3\n\n# Add first subplot\nax_f = fig.add_subplot(m,n,(1,n))\nax_f.plot(x,y)\nax_f.set_title(function_names[0])\n\n# Add derivatives\nfor i in range(n):\n    ax_deriv = fig.add_subplot(m,n,n+1+i)\n    ax_deriv.plot(x,deriv[i])\n    ax_deriv.set_title(function_names[1+i])\n    \n# Tighten layout\nplt.tight_layout()\n\n# Show plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "07-visualization.html#bivariate-functions",
    "href": "07-visualization.html#bivariate-functions",
    "title": "7  Visualization",
    "section": "7.3 Bivariate functions",
    "text": "7.3 Bivariate functions\nIn Python it is also possible to plot a function with two variables, a so-called bivariate function. For example, consider z = f(x,y) = x^2 + y^2 that we would like to visualize on the (x,y)-domain [0,4] \\times [3,8].\nBefore going into the plotting commands we discuss in more detail how to efficiently compute the function values on the specified domain.\nJust as with two-dimensional plotting, the idea is that we want compute the function values on a fine-grained discretization of the desired domain. In two dimensions, we can create such a discretization easily using the mgrid function from NumPy.\nSuppose we discretize [0,4] \\times [3,8] by considering all the integer combinations. We can do this with mgrid by specifying for both dimensions the range that we are interested in using index slicing.\n\n# Note that the end index is not included\nX, Y = np.mgrid[0:5, 3:9]\n\nIf you input two ranges then the output are two matrices. You could also use this function for higher-dimensional problems.\n\nprint(\"X = \\n\", X)\n\nX = \n [[0 0 0 0 0 0]\n [1 1 1 1 1 1]\n [2 2 2 2 2 2]\n [3 3 3 3 3 3]\n [4 4 4 4 4 4]]\n\n\n\nprint(\"Y = \\n\", Y)\n\nY = \n [[3 4 5 6 7 8]\n [3 4 5 6 7 8]\n [3 4 5 6 7 8]\n [3 4 5 6 7 8]\n [3 4 5 6 7 8]]\n\n\nThe matrices X and Y together form a representation of all the integer points in the domain [0,4] \\times [3,8], namely\n\n\\begin{array}{cccccc}\n(0,3) & (0,4) & (0,5) & (0,6) & (0,7) & (0,8)\\\\\n(1,3) & (1,4) & (1,5) & (1,6) & (1,7) & (1,8)\\\\\n(2,3) & (2,4) & (2,5) & (2,6) & (2,7) & (2,8)\\\\\n(3,3) & (3,4) & (3,5) & (3,6) & (3,7) & (3,8)\\\\\n(4,3) & (4,4) & (4,5) & (4,6) & (4,7) & (4,8)\n\\end{array}\n\nTo be precise, the matrix X contains the first element of every coordinate (i,j) \\in [a,b] \\times [c,d], that is, the value i, and Y contains the second element, that is, the value j.\nThe same can be achieved with the function meshgrid() that takes as input the discretized ranges of x and y. Here the matrix X and Y are tranposed compared to the output of mgrid.\n\nx = np.arange(0,5)\ny = np.arange(3,9)\n\nX, Y = np.meshgrid(x,y)\n\nprint(\"X = \\n\", X)\nprint(\"Y = \\n\", Y)\n\nX = \n [[0 1 2 3 4]\n [0 1 2 3 4]\n [0 1 2 3 4]\n [0 1 2 3 4]\n [0 1 2 3 4]\n [0 1 2 3 4]]\nY = \n [[3 3 3 3 3]\n [4 4 4 4 4]\n [5 5 5 5 5]\n [6 6 6 6 6]\n [7 7 7 7 7]\n [8 8 8 8 8]]\n\n\nIf we now want to compute the function values in the points (i,j), we can simply use Z = X**2 + Y**2. Note that ** is a vectorized operation that is pointwise applied when executed on a two-dimensional array. So X**2 gives the squares of all the x-coordinate of all the grid points, and Y**2 the squares of all the y-coordinates. In other words, for a grid point (i,j) we get Z[i,j] = X[i,j]**2 + Y[i,j]**2.\n\n# Compute function values of f(x,y) = x^2 + y^2\nZ = X**2 + Y**2 # For every (i,j), computes i**2 + j**2\n\nprint(\"Z = \\n\", Z)\n\nZ = \n [[ 9 10 13 18 25]\n [16 17 20 25 32]\n [25 26 29 34 41]\n [36 37 40 45 52]\n [49 50 53 58 65]\n [64 65 68 73 80]]\n\n\nWe can also do this by defining the function f.\n\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Define grid\nX, Y = np.mgrid[0:5, 3:9]\n\n# f(X,Y) gives all function values of the grid points.\nprint(\"f(x,y) = \\n\", f(X,Y))\n\nf(x,y) = \n [[ 9 16 25 36 49 64]\n [10 17 26 37 50 65]\n [13 20 29 40 53 68]\n [18 25 34 45 58 73]\n [25 32 41 52 65 80]]\n\n\nWe can create a more fine-grained plot by including more points in the ranges of the x- and y-values. Again, this can be done using slicing notation.\n\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Define grid with step size of 0.2 in x,y-coordinates\nstep = 0.2\nX, Y = np.mgrid[0:2.1:step, 3:4.1:step]\n\n# Print x-values of grid points\nprint(\"X = \\n\", X)\n\n# Print y-values of grid points\nprint(\"Y = \\n\", Y)\n\n# f(X,Y) gives all function values of the grid points.\nprint(\"f(X,Y) = \\n\", f(X,Y))\n\nX = \n [[0.  0.  0.  0.  0.  0. ]\n [0.2 0.2 0.2 0.2 0.2 0.2]\n [0.4 0.4 0.4 0.4 0.4 0.4]\n [0.6 0.6 0.6 0.6 0.6 0.6]\n [0.8 0.8 0.8 0.8 0.8 0.8]\n [1.  1.  1.  1.  1.  1. ]\n [1.2 1.2 1.2 1.2 1.2 1.2]\n [1.4 1.4 1.4 1.4 1.4 1.4]\n [1.6 1.6 1.6 1.6 1.6 1.6]\n [1.8 1.8 1.8 1.8 1.8 1.8]\n [2.  2.  2.  2.  2.  2. ]]\nY = \n [[3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]\n [3.  3.2 3.4 3.6 3.8 4. ]]\nf(X,Y) = \n [[ 9.   10.24 11.56 12.96 14.44 16.  ]\n [ 9.04 10.28 11.6  13.   14.48 16.04]\n [ 9.16 10.4  11.72 13.12 14.6  16.16]\n [ 9.36 10.6  11.92 13.32 14.8  16.36]\n [ 9.64 10.88 12.2  13.6  15.08 16.64]\n [10.   11.24 12.56 13.96 15.44 17.  ]\n [10.44 11.68 13.   14.4  15.88 17.44]\n [10.96 12.2  13.52 14.92 16.4  17.96]\n [11.56 12.8  14.12 15.52 17.   18.56]\n [12.24 13.48 14.8  16.2  17.68 19.24]\n [13.   14.24 15.56 16.96 18.44 20.  ]]\n\n\n\n7.3.1 Contour plot\nOne way to visualize a function f : \\mathbb{R}^2 \\rightarrow \\mathbb{R} is by using a contour plot with plt.contour(). What such a plot does is that it creates a two-dimensional plot where for given values z_0 \\in \\mathbb{R} it plots all points (x,y) \\in \\mathbb{R}^2 for which f(x,y) = z_0 with the same color.\nThe function plt.contour() takes as input the arrays X, Y and Z. The input order is important here. Python plots the point (X[i,i],Y[i,j]) and assigns a common color to all such points with the same function value (i.e., the same Z[i,j]-value).\nThe levels keyword argument determines how many different colors, i.e., z_0-values, are plotted. For this Python uses a so-called colormap that has a shifting scale indicating a shift in the value of z_0. You can plot a color legend with plt.colorbar().\n\n\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Grid parameters\nb = 4\nstep = 0.001\n\n# Define grid [0,b]^2 with given step size\nX, Y = np.mgrid[0:b:step, 0:b:step]\n\n# Create figure\nplt.figure()\n\n# Create contour plot\nplt.contour(X, Y, f(X,Y), levels=10)\n\n# Add labels and title\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Contour plot of function\")\n\n# Show the plot\nplt.colorbar()  # Add a color bar for reference\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nThe same plot as above with 50 color levels is given below.\n\n\n\nShow code generating the plot below\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Grid parameters\nb = 4\nstep = 0.001\n\n# Define grid [0,b]^2 with given step size\nX, Y = np.mgrid[0:b:step, 0:b:step]\n\n# Create figure\nplt.figure()\n\n# Create contour plot\nplt.contour(X, Y, f(X,Y), levels=50)\n\n# Add labels and title\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Contour plot of function\")\n\n# Show the plot\nplt.colorbar()  # Add a color bar for reference\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can also choose to fill up the white space between the different contour lines. Then you should use plt.contourf() instead of plt.contour(). The same plot with 50 color levels and plt.contourf() is given below.\n\n\n\nShow code generating the plot below\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Grid parameters\nb = 4\nstep = 0.001\n\n# Define grid [0,b]^2 with given step size\nX, Y = np.mgrid[0:b:step, 0:b:step]\n\n# Create figure\nplt.figure()\n\n# Create contour plot with 50 levels\nplt.contourf(X, Y, f(X,Y), levels=50)\n\n# Add labels and title\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Contour plot of function\")\n\n# Show the plot\nplt.colorbar()  # Add a color bar for reference\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nFinally, you can change the color chosen by Python using the cmap keyword argument. There are various color maps available; see the documentation.\nBelow we have plotted the figure above with the inferno colormap.\n\n\n\nShow code generating the plot below\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Grid parameters\nb = 4\nstep = 0.001\n\n# Define grid [0,b]^2 with given step size\nX, Y = np.mgrid[0:b:step, 0:b:step]\n\n# Create figure\nplt.figure()\n\n# Create contour plot with 50 levels\nplt.contourf(X, Y, f(X,Y), levels=50, cmap=\"inferno\")\n\n# Add labels and title\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Contour plot of function\")\n\n# Show the plot\nplt.colorbar()  # Add a color bar for reference\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 3D plot\nAnother way of visualizing a bivariate function is using a 3D plot, in which we have an x-, y- and z-axis. This you can do, e.g., with the plot_surface() function. To use this function we have to explicitly create an Axes object (plot), which we call ax, with three axes using ax = plt.axes(projection='3d').\n\n\n# Define function\ndef f(x,y):\n    return x**2 + y**2\n\n# Grid parameters\nb = 4\nstep = 0.001\n\n# Define grid [0,b]^2 with given step size\nX, Y = np.mgrid[0:b:step, 0:b:step]\n\n# Create figure\nfig = plt.figure(figsize=(7,5))\n\n# Create Axes object with three axes\nax = plt.axes(projection='3d')\n\n# Create surface plot\nax.plot_surface(X, Y, f(X,Y), cmap=\"inferno\")\n\n# Add labels and title\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Surface plot of function\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nWe close this chapter by remarking that we have only shown a small fraction of the plotting functionality that Python has to offer. There are many more options to create nice looking plots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "08-probability.html",
    "href": "08-probability.html",
    "title": "8  Probability theory",
    "section": "",
    "text": "8.1 Randomness\nThere are various ways to generate random variables and sets in Python. Throughout this chapter we will rely on basic concepts from probability theory such as probability density function (pdf) and cumulative density function (cdf).\nFurthermore, we will be working with the normal, uniform  and other distributions. Familiarity with those is assumed. Whenever you run into a mathematical or probabilistic aspect you do not understand, please look it up and otherwise ask the teacher about it.\nWe will use functionality from the random subpackage of NumPy. This subpackage is not to be confused with the random package of Python, which has a lot of similar functionality.\nimport numpy as np",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "08-probability.html#randomness",
    "href": "08-probability.html#randomness",
    "title": "8  Probability theory",
    "section": "",
    "text": "8.1.1 Samples\nThe random subpackage has functions that can generate so-called pseudo-random numbers. Informally speaking, these numbers “behave” as random numbers when performing numerical experiments with them, but they are not truly random (which can be an issue, e.g., if you would use such numbers of cryptographic purposes).\nWe next discuss some functions to generate random numbers from well-known distributions.\nnp.random.rand(m,n): Generate an m \\times n two-dimensional array with numbers uniformly drawn form the interval [0,1] (i.e., every number is equally likely).\n\nm,n = 2, 4\n\nM = np.random.rand(m,n)\nprint(M)\n\n[[0.85772641 0.17580051 0.07604031 0.95116006]\n [0.82971756 0.61598337 0.84856947 0.61634003]]\n\n\nWhen m and n are not inputted, a single random number is returned.\n\nx = np.random.rand()\nprint(x)\n\n0.8762477436712724\n\n\nnp.random.randn(m,n): Generates an m \\times n two-dimensional array with numbers drawn from the normal distribution with mean \\mu = 0 and standard deviation \\sigma = 1.\n\nm,n = 3, 5\n\nM = np.random.randn(m,n)\nprint(M)\n\n[[-0.01216961 -0.36948594 -2.9349811   0.4571926   1.72031967]\n [ 0.72400178 -0.14627722 -1.90488267  1.5053437  -0.2385779 ]\n [ 1.02211965  1.33870794  0.73103693  0.3251402  -1.11794456]]\n\n\nWe conclude with sampling numbers from the discrete uniform distribution over a set \\{a,a+1,\\dots,b-1\\}. Here every number i in this discrete interval is generated with equal probability 1/(b - a).\nnp.random.randint(a,b,(m,n)): Generates an m \\times n two-dimensional array with numbers drawn from the discrete uniform distribution on \\{a,a+1,\\dots,b-1\\}.\n\na, b = 4, 10\nm,n = 3, 15\n\nM = np.random.randint(a,b,(m,n))\nprint(M)\n\n[[4 6 8 7 7 8 5 4 8 9 7 5 4 9 6]\n [7 9 6 5 4 6 7 7 5 7 5 8 5 5 7]\n [7 9 5 4 9 8 6 9 5 8 7 5 4 6 6]]\n\n\nFor almost every well-known probability distribution we can get samples from its distribution. See the documentation for a list of all the distributions.\nFinally, we remark that it is also possible to generate samples from different distributions from the same family. As an example, consider a normal distribution with mean \\mu and standard deviations \\sigma. We can generate samples from this distribution with np.random.normal().\nAs input it takes two keyword arguments: loc for the mean \\mu and scale for the standard deviation \\sigma. You can figure this out by inspecting the documentation.\nMost distributions have a loc and or scale keyword input argument, that takes a default value if none is inputted; loc=0 and scale=1 are the defaults for np.random.normal.\n\n\n\nDocumentation for generating normally distributed samples\n\n\n\nmu = 1\nsigma = 2\n\nsamples = np.random.normal(loc=mu, scale=sigma)\n\nprint(samples)\n\n1.6449343027715893\n\n\nThis function also allows vectorized inputs: If we input an array of means and standard deviations, then a sample for every combination is generated.\n\nmu = np.array([0,1,2,2])\nsigma = np.array([1,2,4,2])\n\nsamples = np.random.normal(loc=mu, scale=sigma)\n\nprint(samples)\n\n[ 0.25924594  1.7073092  -2.7038914   2.12464626]\n\n\nFinally, if you want to have m samples from all combinations of location and scale parameters, you can specify this in the size keyword argument by setting it to (m,n) where n is the common length of the location and scale array. For every scale-location combination, the m samples of this combination can be found in a column of the output.\n\nmu = np.array([0,1,2,2])\nsigma = np.array([1,2,4,2])\n\nm = 3\nn = np.size(mu)\n\nsamples = np.random.normal(loc=mu, scale=sigma, size=(m,n))\n\nprint(samples)\n\n[[ 0.35556754 -2.59413638 -6.29769609  2.07853133]\n [-0.45817318  1.0522859  -2.76675983  1.30892729]\n [-0.4468004   1.0616758  -4.57884182  3.4075547 ]]\n\n\n\n\n8.1.2 Subsets\nNext to the generation of random numbers, it is also possible to generate random subsets of elements of a given array using the choice() function. It takes as input an array from which we want to obtain a subset and we an set the size of the subset that we want to have using the size keyword argument.\n\nk = 15\nx = np.arange(0,11,1)\n\nsubset = np.random.choice(x,size=k)\n\nprint(subset)\n\n[ 4  8  9  8  1 10  0  0  7  6  1  9  9  0  8]\n\n\nAs can be seen from the output above, some numbers appear twice in the subset, meaning that choice() samples a subset with replacement. If you want to sample without replacement, you can set the keyword replace to False.\n\nk = 8\nx = np.arange(0,11,1)\n\nsubset = np.random.choice(x,size=k,replace=False)\n\nprint(subset)\n\n[ 4  2  8  5 10  9  3  6]\n\n\nFinally, you can also specify the probability with which every element should be samples using the keyword argument p.\n\nk = 8\n\n# Set {0,1,2,...,9}\nx = np.arange(0,10,1)\n\n# Probabilities for elements in set\nprob = np.array([1/3,1/3,0,0,0,0,0,0,0,1/3])\n\n# Generating random subset\nsubset = np.random.choice(x,size=k,p=prob)\n\n# Only 0, 1, 9 have positive probabilities\nprint(subset)\n\n[9 1 1 1 0 1 1 0]\n\n\nThere is also the permutation() function that returns the elements in an array in a random order. That is, it creates a so-called random permutation of the elements in the array.\n\nx_perm = np.random.permutation(x)\n\nprint(x_perm)\n\n[7 2 1 8 4 0 5 3 6 9]\n\n\nIf you would apply this function on a two-dimensional array, it returns the same array in which the rows are randomly permuted (i.e., the inner lists are randomly permuted).\n\nX = np.arange(0,18,1).reshape(3,6)\n\nx_perm = np.random.permutation(X)\n\nprint(x_perm)\n\n[[12 13 14 15 16 17]\n [ 6  7  8  9 10 11]\n [ 0  1  2  3  4  5]]\n\n\n\n\n8.1.3 Seed\nWhen writing code that involves random numbers of objects, it can sometimes be useful to “fix” the randomness in the script, e.g., when debugging. This can be done by setting a so-called random seed using the seed() function.\nFor sake of comparison, let us first generate two random numbers from [0,1].\n\na = np.random.rand()\nb = np.random.rand()\n\nprint(a,b)\n\n0.6898919561953905 0.7862620817472092\n\n\nIf you rerun the code above it will give different outputs every time. Try this yourself by copying the code into Spyder.\nWe next do the same, but with a fixed seed s using seed(s). Different choices of s fix the randomness in a different way. If you copy the code below into Spyder and rerun it a couple of times, the output will always be the same.\n\n#Set seed to be s = 3\nnp.random.seed(3)\n\na = np.random.rand()\nb = np.random.rand()\n\nprint(a,b)\n\n0.5507979025745755 0.7081478226181048",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "08-probability.html#probability-distributions",
    "href": "08-probability.html#probability-distributions",
    "title": "8  Probability theory",
    "section": "8.2 Probability distributions",
    "text": "8.2 Probability distributions\nThe stats module of SciPy has many built-in probability distributions. Each distribution can be seen as an object on which various methods can be performed (such accessing its probability density function or summary statistics like the mean and median). You should think of an ‘object’ in the context of object oriented programming, see, e.g., here to recall the basics of this paradigm.\n\nimport scipy\n\nIn this section we will focus on continuous probability distributions. SciPy also has many built-in discete probability distributions.\nA list of all continuous distributions that are present in the stats module can be found here; they are so-called stats.rv_continuous objects. We can instantiate a distributional object by using scipy.stats.dist_name where dist_name is the name of a built-in (continuous) probability distribution in the mentioned list.\nMany distributions have input parameters scale and loc that model the scale and location of the distribution, respectively. Depending on the distribution that is considered, these parameters have different meanings.\nAs an example, the normal distribution has probability density function \nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n which is parameterized by \\mu and \\sigma.\nIn Python \\mu is the loc parameter, and \\sigma the scale parameter. To figure out the function of the scale and loc parameter, you can check the documentation (which can be found here for the Normal distribution).\n\n\n\nDocumentation of the normal distribution\n\n\nAll distributions have default values for these parameters, which are typically loc=0 and scale = 1.\n\n# Create normal distribution object with mu=0, sigma=1\ndist_norm = scipy.stats.norm(loc=0, scale=1)\n\nOnce a distribution object has been instantiated, we can use methods (i.e., functions) to obtain various properties of the distribution, such as its probability density function (pdf), cumulative density function (cdf) and summary statistics such as the mean, variance and median (or, more general, quantiles).\nWe give a list of some common methods for a distribution object named dist_name. We start with common functions associated with a probability distribution.\n\ndist_name.pdf(x): Value f(x) where f is the pdf of the distribution.\ndist_name.cdf(x): Value F(x) where F is the cdf of the distribution.\ndist_name.sf(x): Value S(x) where S is the survival function (1 - F) of the distribution.\ndist_name.ppf(alpha): Returns x so that F^{-1}(x) = \\alpha where \\alpha \\in (0,1).\n\n\nx = 1\nprint(dist_norm.pdf(x))\n\n0.24197072451914337\n\n\n\nalpha = 1/2\nprint(dist_norm.ppf(alpha))\n\n0.0\n\n\nAll the above functions are vectorized, in the sense that they can also handle higher-dimensional arrays as input. This is convenient, e.g., for visualizing these functions as the example below illustrates.\n\n\n\nShow code generating the plot below\nimport matplotlib.pyplot as plt\n\n# Define the x range of x-values\nx = np.linspace(-10,10,600)\n\n# Function values of pdf\ny = dist_norm.pdf(x) # Computes pdf values of all elements in x\n\n#Create the figure\nplt.figure()\n\n# Create the plot\nplt.plot(x, y)\n\n# Create labels for axes\nplt.xlabel('x')\nplt.ylabel('Function value')\n\n# Fix the range of the axes\nplt.xlim(-10,10)\nplt.ylim(0,1)\n\n# Add title to the plot\nplt.title('Pdf of Normal distribution with $\\mu=0$ and $\\sigma=1$')\n\n# Add grid to the background\nplt.grid()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also access various summary statistics:\n\ndist_name.mean(): Returns mean of the distribution\ndist_name.var(): Returns variance of the distribution\ndist_name.median(): Returns median of the distribution\n\n\ndist_norm = scipy.stats.norm(loc=0,scale=2)\n\nmean =  dist_norm.mean()\nvariance = dist_norm.var()\nmedian = dist_norm.median()\n\nprint(\"Mean of the distribution is\", mean)\nprint(\"Variance of the distribution is\", variance)\nprint(\"Median of the distribution is\", median)\n\nMean of the distribution is 0.0\nVariance of the distribution is 4.0\nMedian of the distribution is 0.0\n\n\nNote that in the example above the standard deviation equals \\sigma = 2; the variance is then \\sigma^2 = 4.\nFinally it is also possible to to access the support upper and lower bound of a distribution.\n\ndist_name.support() : Returns values a,b of (smallest) interval [a,b] for which all probability mass is contained in it.\n\nLet us first consider the uniform distribution.\n\n\n\nDocumentation of the uniform distribution\n\n\nHere the scale and loc parameters result in a uniform distribution on the interval [loc, loc + scale].\n\n#Uniform distribution on [3,3+4] = [3,7]\ndist_unif = scipy.stats.uniform(3,4) \n\na,b = dist_unif.support()\n\nprint(f\"The distribution is supported on the interval [{a},{b}]\")\n\nThe distribution is supported on the interval [3.0,7.0]\n\n\nSome distributions have an unbounded support. In the case of the Normal distribution, we have a = -\\infty and b = \\infty. For the Exponential Distribution, we have a = 0 and b = \\infty.\n\ndist_norm = scipy.stats.norm(loc=0, scale=1)\n\na,b = dist_norm.support()\n\nprint(f\"The Normal distribution is \\\nsupported on the interval [{a},{b}]\")\n\nThe Normal distribution is supported on the interval [-inf,inf]\n\n\n\ndist_exp = scipy.stats.expon(loc=0,scale=1)\n\na,b = dist_exp.support()\n\nprint(f\"The Exponential distribution is \\\nsupported on the interval [{a},{b}]\")\n\nThe Exponential distribution is supported on the interval [0.0,inf]\n\n\nThe value inf that b has in the example above is in fact a number within NumPy, namely np.inf. Let us check that the upper bound of the exponential distribution is indeed np.inf.\n\nstatement = (b == np.inf)\n\nprint(statement)\n\nTrue\n\n\n\n8.2.1 Distributions as input arguments\nDistributional objects can also serve as input arguments of a function. In that case, you can access the methods of the object inside the function.\nSuppose we want to write a function that outputs a message saying whether or not the mean or the median of a distribution is larger. We can do this as follows for an arbitrary distribution.\n\ndef mean_median(dist):\n    if dist.mean() &lt; dist.median():\n        return \"The mean is smaller than the median\"\n    elif dist.mean() == dist.median():\n        return \"The mean and median are equal.\"\n    elif dist.mean() &gt; dist.median():\n        return \"The mean is larger than the median.\"\n\nIn the function above, the input argument dist is a distribution object whose methods mean() and median() we access within the function.\n\ndist_norm = scipy.stats.norm(loc=1,scale=2)\n\ncomparison  = mean_median(dist_norm)\nprint(comparison)\n\nThe mean and median are equal.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "09-data-fitting.html",
    "href": "09-data-fitting.html",
    "title": "9  Statistics and data fitting",
    "section": "",
    "text": "9.1 Correlation coefficients\nSuppose we have collected two features of a group of n people, their weight (kg) and height (cm), in arrays x = [x_0,\\dots,x_{n-1}] and y = [y_0,\\dots,y_{n-1}], respectively. One might expect some correlation between these two features, as taller people typically weigh a bit more than shorter people. One way to quantify such relations is to compute a correlation coefficient of the data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistics and data fitting</span>"
    ]
  },
  {
    "objectID": "09-data-fitting.html#correlation-coefficients",
    "href": "09-data-fitting.html#correlation-coefficients",
    "title": "9  Statistics and data fitting",
    "section": "",
    "text": "9.1.1 Pearson coefficient\nThe Pearson correlation coefficient for these arrays is defined as\n\nP(x,y) = \\frac{\\displaystyle \\sum_{i=0}^{n-1} (x_i - \\bar{x})(y_i - \\bar{y})}{\\displaystyle \\sqrt{\\sum_{i=0}^{n-1} (x_i - \\bar{x})^2\\sum_{i=0}^{n-1} (y_i - \\bar{y})^2}}\n\nwhere \\bar{x} and \\bar{y} are the means (or averages) of the vectors x and y, respectively. It holds that P(x,y) \\in [-1,1] with the interpretation that the larger |P(x,y)| is, the more correlation (positive or negative) the arrays have.\nNumPy has a built-in function corrcoef() to compute the Pearson coefficient of the arrays x and y. In fact, this function works in a vectorized way. If we input a two-dimensional array, then this function computes the Pearson coefficient for every pair of rows of the array.\n\nimport numpy as np\nimport scipy.stats\n\n# Heights\nx = np.array([187, 174, 179, 192, 188, 160, 179,  168, 168, 174])\n\n# Weights\ny = np.array([94, 88, 91, 96, 95, 80, 91, 84, 85,  86])\n\ndata = np.vstack((x,y)) # Store data in two-dimensional array\n\nP = np.corrcoef(data)\n\nprint(P)\n\n[[1.         0.98888612]\n [0.98888612 1.        ]]\n\n\nNote that there will always be ones on the diagonal as the first row is perfectly correlated with itself, and that this matrix is symmetric since P(X,Y) = P(Y,X). Let us also add a feature (age) which is not really correlated with the other two features and recompute the Pearson coefficients.\n\n# Heights\nx = np.array([187, 174, 179, 192, 188, 160, 179, 168, 168, 174])\n\n# Weights\ny = np.array([94, 88, 91, 96, 95, 80, 91, 84, 85,  86])\n\n# Age\na = np.array([23, 23, 23, 24, 25, 24, 24, 23, 24, 23])\n\ndata = np.vstack((x,y,a))\n\nP = np.corrcoef(data)\n\nprint(P)\n\n[[1.         0.98888612 0.21342006]\n [0.98888612 1.         0.24120908]\n [0.21342006 0.24120908 1.        ]]\n\n\nAs you can see, the correlation coefficients of the height-age (\\approx 0.21) and heigh-weight (\\approx 0.24) combinations is rather low.\nThe stats module of SciPy also has a built-in function pearsonr() to compute the Pearson coefficient of two arrays of feature data. This function also performs some additional hypothesis testing on the data, but can unfortunately not handle two-dimensional arrays as input.\nIf you only want to compute the coefficient for two features, then this function is also suitable, but if you want to compute a correlation coefficient matrix like above, corrcoef() is the better choice.\n\n\n9.1.2 Spearman rank coefficient\nAnother famous correlation coefficient is the Spearman rank coefficient. Whereas the Pearson correlation is useful when you expect a linear relation between the two features under consideration, the Spearman coefficient is more useful when you expect only a monotone, but non necessarily linear, relation. Monotone here means that when the value of the first feature becomes larger, the value of the second feature also becomes larger.\nThere are other factors that determine whether the Pearson or Spearman coefficient is more suitable, but we omit those here.\nSuppose we have collected data about the number of hours that students study for an exam and their grade. One might expect that students who have studied more hours also have obtained a higher grade, but it is not to be expected that this relation is linear. For example, studying a double number of hours is not always guaranteed to double your grade.\nWe have collected some data in the arrays hours and grade with hours[i] denoting the number of hours that student i studied, and grade[i] the grade this student obtained. That data is visualized below as well. Note that in the figure one can see a monotone relation between the features (study hours and grade), but this relation is not linear.\n\n# Study hours \nhours = np.array([1, 2, 2, 4, 3, 5, 7, 8, 6, 10, 14, 12,  15, 18, 20])\n\n# Grades \ngrade = np.array([1.3,3,2.4,3,3.5,3.8,5,7,7,8,8.3,8,9,8.4,9.5])\n\n\n\n\nShow code generating the plot below\nimport matplotlib.pyplot as plt\n\n# Create figure\nplt.figure()\n\n# Create scatter plot of data points\nplt.scatter(hours,grade)\n\n# Set axes limits\nplt.xlim(0,np.max(hours)+1)\nplt.ylim(0,np.max(grade)+1)\n\n# Set axes labels\nplt.xlabel(\"Study hours\")\nplt.ylabel(\"Grade\")\n\n# Set title\nplt.title(\"Study hours vs. obtained grade\")\n\n# Create grid\nplt.grid()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThe determine the Spearman rank coefficient, we first compute the ranks of the data of the features. The smallest value in a feature array gets rank 1, the second smallest rank 2, etc. This can be done with the rankdata() function from the stats module.\n\n# Ranks of study hours values\nranks_hours = scipy.stats.rankdata(hours)\n\nprint(ranks_hours)\n\n[ 1.   2.5  2.5  5.   4.   6.   8.   9.   7.  10.  12.  11.  13.  14.\n 15. ]\n\n\nNote that when a number appears multiple times in the array, then an average rank is computed.\n\n# Ranks of grade values\nranks_grades = scipy.stats.rankdata(grade)\n\nprint(ranks_grades)\n\n[ 1.   3.5  2.   3.5  5.   6.   7.   8.5  8.5 10.5 12.  10.5 14.  13.\n 15. ]\n\n\nAfter having computed the ranks, the Spearman rank coefficient is obtained by computing the Pearson correlation coefficients of the rank arrays.\n\nrank_data = np.vstack((ranks_hours,ranks_grades))\n\nS = np.corrcoef(rank_data)\n\nprint(S)\n\n[[1.         0.98118437]\n [0.98118437 1.        ]]\n\n\nThere is a built-in function spearmanr() that carries out the two steps mentioned above. This function is vectorized in the sense that if we put in a two-dimensional array, then every column is interpreted as the data corresponding to a feature, and the correlation between different columns is computed. If the data of a feature is given as a row, we can set the axis keyword argument to axis=1.\n\n# Study hours \nhours = np.array([1, 2, 2, 4, 3, 5, 7, 8, 6, 10, 14, 12,  15, 18, 20])\n\n# Grades \ngrade = np.array([1.3,3,2.4,3,3.5,3.8,5,7,7,8,8.3,8,9,8.4,9.5])\n\ndata = np.vstack((hours,grade))\n\nS = scipy.stats.spearmanr(data,axis=1)\n\nprint(S)\n\nSignificanceResult(statistic=0.9811843713228874, pvalue=1.1432541280704027e-10)\n\n\nThis function spearmanr() outputs the Spearman rank coefficient and a p-value. You can read about the latter in the documentation. The rank coefficient is stored in the statistic attribute.\n\nprint(S.statistic)\n\n0.9811843713228874\n\n\nAlternatively, you can suppress the p-value output argument to only get the rank coefficient.\n\nS, _ = scipy.stats.spearmanr(data,axis=1)\n\nprint(S)\n\n0.9811843713228874\n\n\n\n\n9.1.3 Other coefficients\nThere are many other correlation coefficients that can be computed with Python, see here for a list. A large collection of statistical tests has also been implemented in the stats module.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistics and data fitting</span>"
    ]
  },
  {
    "objectID": "09-data-fitting.html#data-fitting",
    "href": "09-data-fitting.html#data-fitting",
    "title": "9  Statistics and data fitting",
    "section": "9.2 Data fitting",
    "text": "9.2 Data fitting\nIn this section we will see various ways in which you can compute a function that fits given data best, namely regression-based problems, (polynomial) interpolation and distributional fitting.\n\n9.2.1 Regression\nIn a regression model, we are given a relation of the form\n\ny_i = f(x_i,\\beta) + \\epsilon_i\n\nwhere f : \\mathbb{R}^{n + k} \\rightarrow \\mathbb{R} is a known function, (x_i,y_i) known data points for i = 0,\\dots,m-1, with x_i = [x_{i0},\\dots,x_{(n-1)0}] \\in \\mathbb{R}^n and y_i \\in \\mathbb{R}. The term \\epsilon_i is an unknown error term that is often assumed to be normally distributed. Its exact distribution is not relevant at this point, because we assume it is unknown.\nThe goal is to determine a vector \\beta = [\\beta_0,\\dots,\\beta_{k-1}] \\in \\mathbb{R}^k that minimizes a given error function. The most well-known choice here is to minimize the sum of the squared errors, i.e., to find a solution to the problem\n\n\\min_{\\beta} \\sum_{i=0}^{m-1} \\epsilon_i^2 = \\min_{\\beta} \\sum_{i=0}^{m-1} (y_i - f(x_i,\\beta))^2.\n\nNote that \\beta = [\\beta_0,\\dots,\\beta_{k-1}] is the only unknown in the right hand side expression above. In other words, this problem tries to find the least squares solution to the system of m equations given by \ny_i - f(x_i,\\beta) = 0\n for i =0,\\dots,m-1. An exact solution does not exist because of the (unknown) error terms \\epsilon_i.\nLet us look at a simple form of linear regression where n = 1 and k = 2. That is, we have f: \\mathbb{R} \\rightarrow \\mathbb{R} defined by f(x) = \\beta_0 + \\beta_1x. Suppose we are given data points (x_i,y_i) \\in \\mathbb{R}^2 for i = 0,\\dots,m-1. Note that x_i is a scalar and not an array in this case, because n = 1. We are looking for a \\beta = [\\beta_0,\\beta_1] that solves the system \n\\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\\\\n1 & x_m\n\\end{pmatrix}\\begin{pmatrix}\\beta_0 \\\\ \\beta_1\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}.\n Because of the error term in the regression model (and the fact that this system is overdetermined), we compute a least squares solution. We have already seen various functions that can do this for us, in particular least_squares from SciPy’s optimize module.\nRecall from Chapter 6 that least_squares takes as input an array of functions g_0(\\beta),\\dots,g_{m-1}(\\beta) and then minimizes over \\beta the expression \\sum_i g_i(\\beta)^2. In our case we have g_i(\\beta) = y_i - f(x_i,\\beta) = y_i - (\\beta_0 + \\beta_1x_i).\nBelow we create the function model() whose output is the array \n\\begin{pmatrix}\ny_0 - f(x_0,\\beta) \\\\\ny_1 - f(x_1,\\beta) \\\\\n\\vdots \\\\\ny_{m-1} - f(x_{m-1},\\beta)\n\\end{pmatrix}.\n This function will serve as the input for least_squares(). To keep the code clean, we create a separate Python function for f.\n\nimport scipy.optimize as optimize\n\n# Function f\ndef f(x,beta):\n    return (beta[0] + beta[1]*x)\n    \n# System of error terms\ndef model(beta,x,y): # beta is first input here; later optimized over\n    return y - f(x,beta)\n\nIt is important that \\beta is the first input argument of model() as this will be the array that we optimize over when looking for a least squares solution. For the function f, we could have also reversed the input arguments.\nWe next generate some synthetic (x_i,y_i) data points.\n\n# Fix random seed\nnp.random.seed(42)\n\n# Number of data points\nm = 10\n\n# Create points (1,1), (2,2), ..., (m,m) with some random noise.\nx = np.arange(1,m+1) + 0.25*np.random.randn(m)\ny = np.arange(1,m+1) + 0.25*np.random.randn(m)\n\nNext, we perform least_squares on the linear model define above. Recall that this function needs an initial guess for the parameters in \\beta to be fitted. Also, we need to use the args keyword argument to specify x and y, which are the additional input arguments of our model() function that are fixed (i.e., are not optimized over).\n\n# Set initial guess\nguess = np.array([2,2])\n\n# Perform least squares method\nresult = optimize.least_squares(model,x0=guess,args=(x,y))\n\n# Print fitted parameters\nprint(result)\n\n     message: `gtol` termination condition is satisfied.\n     success: True\n      status: 1\n         fun: [ 9.163e-03  1.787e-01  1.752e-01 -5.660e-01 -7.205e-02\n                2.321e-01 -3.143e-01  2.312e-01  2.441e-01 -1.181e-01]\n           x: [-2.340e-01  9.865e-01]\n        cost: 0.3339323530848278\n         jac: [[-1.000e+00 -1.124e+00]\n               [-1.000e+00 -1.965e+00]\n               ...\n               [-1.000e+00 -8.883e+00]\n               [-1.000e+00 -1.014e+01]]\n        grad: [ 2.220e-16  8.122e-09]\n  optimality: 8.121683325867934e-09\n active_mask: [ 0.000e+00  0.000e+00]\n        nfev: 3\n        njev: 3\n\n\nThe values in \\beta can be found in the x attribute\n\nbeta = result.x\n\nprint(beta)\n\n[-0.23404544  0.98652277]\n\n\nFinally, we plot the (x_i,y_i) data points together with the fitted line f(x) = \\beta_0 + \\beta_1 x to visually inspect our fitting procudure.\n\n\n\nShow code generating the plot below\n# Determine x- and y-values for the fitted line\nx_line = np.linspace(0,11,100)\ny_line = f(x_line,beta)\n\n\n# Create figure\nplt.figure()\n\n# Scatter plot of data points\nplt.scatter(x,y,label=\"Data points\")\n\n# Plot fitted line\nplt.plot(x_line,y_line,c='r',label=\"Fitted model\")\n\n# Set axes limits\nplt.xlim(0,m+1)\nplt.ylim(0,m+1)\n\n# Set axes labels\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Set title\nplt.title(\"Fitting linear model\")\n\n# Create grid\nplt.grid()\n\n# Create legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nOne should recall that because of the fact that f is linear in the parameters in \\beta, we could have also used the linalg.lstsq() function from Numpy. Although you do not need to fully understand the code below, we include it here for sake of reference.\n\n# Create left hand side matrix of system above\nA = np.vstack((np.ones(m),x)).T\n\n# Right hand side vector of system\nb = y\n\n# We add rcond=None to avoid a warning raised by Python\nresult = np.linalg.lstsq(A,b,rcond=None)[0] # First output is our beta\n\nprint(result)\n\n[-0.23404544  0.98652277]\n\n\nMost importantly, least_squares is also able to handle non-linear function f in our regression framework, for example \ny_i = \\left(\\frac{\\beta_0 + \\sqrt{x_i}}{\\beta_1\\sqrt{x_i}} \\right)^2 + \\epsilon_i\n for i = 0,\\dots,m-1. Note that also here n = 1 and k = 2.\nCarrying out the same steps as above we obtain the following code to fit this function on given synthetic (x_i,y_i) data points.\n\n# Fix random seed\nnp.random.seed(42)\n\n# Function f\ndef f(x,beta):\n    return ((beta[0] + np.sqrt(x))/(beta[1]*np.sqrt(x)))**2\n\n# Define the non-linear model\ndef model(beta,x,y):\n    return y - f(x,beta)\n\n# Number of data points\nm = 10\n\n# These will be the choice of beta for which we generate the data\nbeta_true = np.array([4,2])\n\n#Generate synthetic data points with some random noise.\nx = np.arange(1,m+1) \ny = f(x,beta_true) + 0.25*np.random.randn(m) \n\n# Set initial guess\nguess = np.array([1,1])\n\n# Perform least squares method on x,y data\nresult = optimize.least_squares(model,x0=guess,args=(x,y))\n\n# Fitted parameters\nbeta_fit = result.x\n\nprint(beta_fit)\n\n[3.49914009 1.78657584]\n\n\n\n\n\nShow code generating the plot below\n# Determine x- and y-values for the fitted line\nx_line = np.linspace(0.01,11,100)\ny_line = f(x_line,beta_fit)\n\n# Create figure\nplt.figure()\n\n# Scatter plot of data points\nplt.scatter(x,y,label=\"Data points\")\n\n# Plot fitted line\nplt.plot(x_line,y_line,c='r',label=\"Fitted model\")\n\n# Set axes limits\nplt.xlim(0,m+1)\nplt.ylim(0,m+1)\n\n# Set axes labels\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Set title\nplt.title(\"Fitting nonlinear model\")\n\n# Create grid\nplt.grid()\n\n# Create legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Interpolation\nIf we want to create a function that goes exactly through given data points, we can do this with interpolation. Suppose we have some data points from the function f(x) = \\sin(x) given below.\n\ndef f(x):\n    return np.sin(x)\n    \n# Data points\nx = np.array([0, 1, 3, 5, 6, 8, 10])\ny = f(x)\n\nLet us plot the sine function and the data points that we created.\n\n# x-range for plotting\nx_plot = np.linspace(np.min(x), np.max(x), 1000) # Define range based on data points\n\n# Plotting sine function\nplt.plot(x_plot, f(x_plot), label='Sine function')\n\n# Plotting data points\nplt.scatter(x, y, label='Data points')\n\n# Set title\nplt.title('Data observations from sine function')\n\n# Set legend\nplt.legend()\n\n# Create grid\nplt.grid()\n\n# Show figure\nplt.show()\n\n\n\n\n\n\n\n\nSuppose now that we would only know the data points. Polynomial interpolation asks for a piecewise polynomial that passes through all the data points, i.e., a function that is is a polynomial between any two consecutive data points. This is also often referred to as spline interpolation.\nSciPy has a built-in function make_interp_spline() in its interpolate module that can yield such a polynomial. It takes as first two inputs arrays the x- and y-coordinates it should pass through.\nFurthermore, the keyword argument k allows you to choose the degree of the polynomial on every segment formed by two consecutive data points.\nBelow we create an interpolation that is a linear between any two data point, i.e., we have k = 1. In other words, this form of interpolation simply connects consecutive data points with a straight line segment.\nThe make_interp_spline() function creates an object that acts as a function, i.e., we can input a scalar or array into it and get back the function values in inputted points.\n\nimport scipy.interpolate as interpolate\n\n# Creates a so-called BSpline object\nlinear_spline = interpolate.make_interp_spline(x,y,k=1)\n\n# Object can be evaluated in vectorized fashion\na = np.array([1,2,3])\nprint(linear_spline(a))\n\n[0.84147098 0.4912955  0.14112001]\n\n\nLet us compare the interpolation polynomial with the original sine function in a figure.\n\n\n\nShow code generating the plot below\n# x-range for plotting\nx_plot = np.linspace(0, 10, 1000)\n\n# Plotting sine function\nplt.plot(x_plot, f(x_plot), label='Sine function')\n\n# Plotting first degree interpolation polynomial\nplt.plot(x_plot, linear_spline(x_plot), label='Piecewise linear spline')\n\n# Plotting data points\nplt.scatter(x, y, label='Data points')\n\n# Set title\nplt.title('Data observations from sine function')\n\n# Set legend\nplt.legend()\n\n# Create grid\nplt.grid()\n\n# Show figure\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n9.2.3 Distributional fitting\nIt is possible to fit the parameters of a known distribution to given data samples using the fit() function of a distribution object. For example, suppose you suspect your data comes from a normal distribution, but do not know its mean and standard deviation.\nBelow we generate some data from a normal distribution.\n\n# Fix randomness\nnp.random.seed(3)\n\n# Samples from normal distribution with given mean and standard dev.\nn = 1000\nsamples = np.random.normal(loc=5,scale=3,size=n)\n\nNow pretend we are given x but do not know the loc and scale parameters that were used to create this array. We can fit the data in x on a normal distribution with the fit() function from the stats module. The output of this function is a tuple with the fitted parameters of the distribution, typically the first one being the location and the second one the scale parameter.\nThe syntax for the fit() function is scipy.stats.distribution_name.fit() where distribution_name is the name of the distribution that we want to fit the data on; see here all the available options.\nLet us fit the data on a normal distribution, with norm as choice for distribution_name.\n\nparameters = scipy.stats.norm.fit(samples)\n\nprint(parameters)\n\n(5.051852998942502, 3.0252056971538166)\n\n\n\nprint(\"Estimated mean is\", parameters[0])\nprint(\"Estimated variance is\", parameters[1])\n\nEstimated mean is 5.051852998942502\nEstimated variance is 3.0252056971538166\n\n\nIf you know one of the scale or location parameters, you can fix these using the floc or fscale keyword arguments. For example, suppose we know that the mean of the data that the distribution was generated from is equal to 5, then we can set floc=5.\n\nmu, sigma = scipy.stats.norm.fit(x, floc=5)\n\nprint(\"Estimated mean is\", mu)\nprint(\"Estimated variance is\", sigma)\n\nEstimated mean is 5\nEstimated variance is 3.3806170189140663\n\n\nThe fit function uses (as default) the maximium likelihood esitmation method to determine the distributional parameters that fit the data best.\nTo inspect whether the returned fitted parameters accurately represents the data, we can visualize the data samples and the fitted distribution in a histogram. We plot the samples using plt.hist(). We have seen this function before in one of the exercises.\nThe first input argument of the hist() function is the list of samples for which we want to create the histogram. The bins keyword argument specifies the number of bars in the histogram, and density=True rescales the histogram so that the total area of the bars equal 1 (which is the same value you get by integrating the area under the probability density function of a distribution).\n\n# Fit data\nmu, sigma = scipy.stats.norm.fit(samples)\n\n# Round coefficients\nmu = np.around(mu,decimals=2)\nsigma = np.around(sigma,decimals=2)\n\n# Create an array of x values for plotting the PDF\nx = np.linspace(np.min(samples), np.max(samples), 100)\n\n# Create figure\nplt.figure()\n\n# Compute PDF-values of elemetns in x for fitted normal distribution\npdf_norm_fit = scipy.stats.norm.pdf(x, mu, sigma)\n\n# Plot the histogram of the data\nplt.hist(samples, bins=30, density=True, label='Sample data')\n\n# Plot the fitted normal distribution\nplt.plot(x, pdf_norm_fit, label=f'Normal fit: mu={mu}, std={sigma}')\n\n# Add labels and legend\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\n\n# Show plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistics and data fitting</span>"
    ]
  },
  {
    "objectID": "A-errata.html",
    "href": "A-errata.html",
    "title": "Appendix A — Corrections and changes",
    "section": "",
    "text": "A.1 Section 9.2.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Corrections and changes</span>"
    ]
  },
  {
    "objectID": "A-errata.html#section-9.2.1",
    "href": "A-errata.html#section-9.2.1",
    "title": "Appendix A — Corrections and changes",
    "section": "",
    "text": "Added explanation of how to generate vectorized samples when multiple location-scale combinations are inputted.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Corrections and changes</span>"
    ]
  },
  {
    "objectID": "A-errata.html#section-3.2.2",
    "href": "A-errata.html#section-3.2.2",
    "title": "Appendix A — Corrections and changes",
    "section": "A.2 Section 3.2.2",
    "text": "A.2 Section 3.2.2\n\nmgrid() explanation is moved to Chapter 5.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Corrections and changes</span>"
    ]
  },
  {
    "objectID": "A-errata.html#section-4.3.1",
    "href": "A-errata.html#section-4.3.1",
    "title": "Appendix A — Corrections and changes",
    "section": "A.3 Section 4.3.1",
    "text": "A.3 Section 4.3.1\n\nCorrected the explanation of np.argsort().",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Corrections and changes</span>"
    ]
  },
  {
    "objectID": "A-errata.html#section-3.5",
    "href": "A-errata.html#section-3.5",
    "title": "Appendix A — Corrections and changes",
    "section": "A.4 Section 3.5",
    "text": "A.4 Section 3.5\n\nCorrected M_repeat = np.tile(M,3) by M_repeat = np.repeat(M,3)\nCorrected vstack(a,b) to vstack((a,b)).\nCorrected hstack(a,b) to hstack((a,b)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Corrections and changes</span>"
    ]
  },
  {
    "objectID": "B-function-basics.html",
    "href": "B-function-basics.html",
    "title": "Appendix B — Function basics",
    "section": "",
    "text": "B.1 Output arguments\nSuppose we have a function that takes two inputs, and yields four outputs.\ndef f(a,b):\n    return a + b, a - b, a*b, a/b\n\na = 5\nb = 3\nThere are various ways to get one or more specific outputs from Python.\n# Returns all output variables in tuple\noutput = f(a,b)\n\nprint(output)\n\n(8, 2, 15, 1.6666666666666667)\n# Store outputs in variables w, x, y and z\nw, x, y, z = f(a,b)\n\nprint(w, x, y, z)\n\n8 2 15 1.6666666666666667\nWe can suppress one or more output arguments using _.\n# Only store first and last output\nw, _, _, z = f(a,b)\n\nprint(w, z)\n\n8 1.6666666666666667\nIf we only want the first output, and suppress the other ones, we can use *_\n# Only store first output\nx, *_ = f(a,b) # This is the same as x, _, _, _ = f(a,b)\n\nprint(x)\n\n8\nIf we only want the first two outputs, and suppress the remaining ones, we can do something similar.\n# Only store first output\nw, x, *_ = f(a,b) # This is the same as w, x, _, _ = f(a,b)\n\nprint(w,x)\n\n8 2\nIf we only want the last output, we can do the following.\n# Only store last output\n*_, z = f(a,b) # This is the same as _, _, _, z = f(a,b)\n\nprint(z)\n\n1.6666666666666667\nAlso here, if we would like the last two outputs, we can do the following.\n# Only store last output\n*_, y, z = f(a,b) # This is the same as _, _, y, z = f(a,b)\n\nprint(y,z)\n\n15 1.6666666666666667",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Function basics</span>"
    ]
  }
]